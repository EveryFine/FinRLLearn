{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Stock NeurIPS2018 Part 2. Train\n",
    "This series is a reproduction of *the process in the paper Practical Deep Reinforcement Learning Approach for Stock Trading*. \n",
    "\n",
    "This is the second part of the NeurIPS2018 series, introducing how to use FinRL to make data into the gym form environment, and train DRL agents on it.\n",
    "\n",
    "Other demos can be found at the repo of [FinRL-Tutorials]((https://github.com/AI4Finance-Foundation/FinRL-Tutorials))."
   ],
   "id": "85d3c001025cf151"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Part 1. Install Packages",
   "id": "48237534fad1d9c6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:54:48.178331Z",
     "start_time": "2024-12-02T13:54:48.175297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import gym\n",
    "import pandas as pd\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "\n",
    "check_and_make_directories([TRAINED_MODEL_DIR])"
   ],
   "id": "511ff5d8c7e9b891",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Part 2. Build A Market Environment in OpenAI Gym-style",
   "id": "d77ce29ff71bafa0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![rl_diagram_transparent_bg.png](data:image/png;base64,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)",
   "id": "cd042a0a7b3800af"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The core element in reinforcement learning are **agent** and **environment**. You can understand RL as the following process: \n",
    "\n",
    "The agent is active in a world, which is the environment. It observe its current condition as a **state**, and is allowed to do certain **actions**. After the agent execute an action, it will arrive at a new state. At the same time, the environment will have feedback to the agent called **reward**, a numerical signal that tells how good or bad the new state is. As the figure above, agent and environment will keep doing this interaction.\n",
    "\n",
    "The goal of agent is to get as much cumulative reward as possible. Reinforcement learning is the method that agent learns to improve its behavior and achieve that goal."
   ],
   "id": "ff05078eb1239add"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To achieve this in Python, we follow the OpenAI gym style to build the stock data into environment.\n",
    "\n",
    "state-action-reward are specified as follows:\n",
    "\n",
    "* **State s**: The state space represents an agent's perception of the market environment. Just like a human trader analyzing various information, here our agent passively observes the price data and technical indicators based on the past data. It will learn by interacting with the market environment (usually by replaying historical data).\n",
    "\n",
    "* **Action a**: The action space includes allowed actions that an agent can take at each state. For example, a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
    "selling, holding, and buying. When an action operates multiple shares, a ∈{−k, ..., −1, 0, 1, ..., k}, e.g.. \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
    "\n",
    "* **Reward function r(s, a, s′)**: Reward is an incentive for an agent to learn a better policy. For example, it can be the change of the portfolio value when taking a at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio values at state s′ and s, respectively\n",
    "\n",
    "\n",
    "**Market environment**: 30 constituent stocks of Dow Jones Industrial Average (DJIA) index. Accessed at the starting date of the testing period."
   ],
   "id": "42f184d4dfe32113"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Read data\n",
    "\n",
    "We first read the .csv file of our training data into dataframe."
   ],
   "id": "b39092aab48b6e1d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:54:52.568474Z",
     "start_time": "2024-12-02T13:54:52.234894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train = pd.read_csv('train_data_dow30_1989-01-01_2022-07-01.csv')\n",
    "train = train.set_index(train.columns[0])\n",
    "train.index.names = ['']\n",
    "train"
   ],
   "id": "fb56410bc441da24",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "            date   tic        open        high         low       close  \\\n",
       "                                                                         \n",
       "0     1990-01-02  AAPL    0.262129    0.332589    0.334821    0.314732   \n",
       "0     1990-01-02  AMGN    0.754523    1.072917    1.072917    1.026042   \n",
       "0     1990-01-02   AXP    4.953164    8.979757    9.011943    8.915386   \n",
       "0     1990-01-02    BA   11.043628   20.500000   20.500000   19.791668   \n",
       "0     1990-01-02   CAT    3.208576    7.359375    7.359375    7.234375   \n",
       "...          ...   ...         ...         ...         ...         ...   \n",
       "8187  2022-06-30   TRV  161.338013  169.130005  169.500000  164.610001   \n",
       "8187  2022-06-30   UNH  497.025818  513.630005  516.320007  513.549988   \n",
       "8187  2022-06-30    VZ   43.047783   50.750000   51.220001   50.669998   \n",
       "8187  2022-06-30   WBA   31.923557   37.900002   39.669998   39.520000   \n",
       "8187  2022-06-30   WMT   39.213158   40.526669   40.686668   40.293331   \n",
       "\n",
       "           volume  day      macd     boll_ub     boll_lb     rsi_30  \\\n",
       "                                                                      \n",
       "0     183198400.0  1.0 -0.020410    0.407460    0.269995  37.182693   \n",
       "0      19267200.0  1.0 -0.037307    1.269225    0.980255  44.088693   \n",
       "0       4301237.0  1.0 -0.113536    9.354541    7.880800  50.163415   \n",
       "0       2042400.0  1.0 -0.012113   20.580973   18.739861  53.656871   \n",
       "0       2910400.0  1.0 -0.045713    7.750962    6.961538  47.910104   \n",
       "...           ...  ...       ...         ...         ...        ...   \n",
       "8187    1336100.0  3.0 -2.374215  180.224943  156.931054  44.639520   \n",
       "8187    3568900.0  3.0  3.243815  522.217561  447.697438  54.941784   \n",
       "8187   18880500.0  3.0  0.170573   52.597046   48.530953  50.161884   \n",
       "8187   15611700.0  3.0 -0.682825   44.006490   38.629509  39.126940   \n",
       "8187   19166100.0  3.0 -0.903056   42.326373   38.975627  39.029583   \n",
       "\n",
       "          cci_30      dx_30  close_30_sma  close_60_sma    vix  \n",
       "                                                                \n",
       "0     -57.773632  14.795458      0.358333      0.385603  17.24  \n",
       "0    -108.941827  10.538710      1.152865      1.136372  17.24  \n",
       "0      44.429024  19.240226      8.751240      9.057539  17.24  \n",
       "0     109.434371  42.703571     19.641667     19.314583  17.24  \n",
       "0     -16.224189   0.407785      7.323958      7.242448  17.24  \n",
       "...          ...        ...           ...           ...    ...  \n",
       "8187  -44.912598   8.641378    170.339999    173.666833  29.42  \n",
       "8187  134.358930  26.659363    486.779667    500.980667  29.42  \n",
       "8187   21.317367  13.348827     50.415666     50.484500  29.42  \n",
       "8187 -160.885298  26.648574     41.693666     43.023000  29.42  \n",
       "8187  -52.314867  19.015585     40.883111     46.010333  29.42  \n",
       "\n",
       "[204700 rows x 17 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tic</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>day</th>\n",
       "      <th>macd</th>\n",
       "      <th>boll_ub</th>\n",
       "      <th>boll_lb</th>\n",
       "      <th>rsi_30</th>\n",
       "      <th>cci_30</th>\n",
       "      <th>dx_30</th>\n",
       "      <th>close_30_sma</th>\n",
       "      <th>close_60_sma</th>\n",
       "      <th>vix</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1990-01-02</td>\n",
       "      <td>AAPL</td>\n",
       "      <td>0.262129</td>\n",
       "      <td>0.332589</td>\n",
       "      <td>0.334821</td>\n",
       "      <td>0.314732</td>\n",
       "      <td>183198400.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.020410</td>\n",
       "      <td>0.407460</td>\n",
       "      <td>0.269995</td>\n",
       "      <td>37.182693</td>\n",
       "      <td>-57.773632</td>\n",
       "      <td>14.795458</td>\n",
       "      <td>0.358333</td>\n",
       "      <td>0.385603</td>\n",
       "      <td>17.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1990-01-02</td>\n",
       "      <td>AMGN</td>\n",
       "      <td>0.754523</td>\n",
       "      <td>1.072917</td>\n",
       "      <td>1.072917</td>\n",
       "      <td>1.026042</td>\n",
       "      <td>19267200.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.037307</td>\n",
       "      <td>1.269225</td>\n",
       "      <td>0.980255</td>\n",
       "      <td>44.088693</td>\n",
       "      <td>-108.941827</td>\n",
       "      <td>10.538710</td>\n",
       "      <td>1.152865</td>\n",
       "      <td>1.136372</td>\n",
       "      <td>17.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1990-01-02</td>\n",
       "      <td>AXP</td>\n",
       "      <td>4.953164</td>\n",
       "      <td>8.979757</td>\n",
       "      <td>9.011943</td>\n",
       "      <td>8.915386</td>\n",
       "      <td>4301237.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.113536</td>\n",
       "      <td>9.354541</td>\n",
       "      <td>7.880800</td>\n",
       "      <td>50.163415</td>\n",
       "      <td>44.429024</td>\n",
       "      <td>19.240226</td>\n",
       "      <td>8.751240</td>\n",
       "      <td>9.057539</td>\n",
       "      <td>17.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1990-01-02</td>\n",
       "      <td>BA</td>\n",
       "      <td>11.043628</td>\n",
       "      <td>20.500000</td>\n",
       "      <td>20.500000</td>\n",
       "      <td>19.791668</td>\n",
       "      <td>2042400.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.012113</td>\n",
       "      <td>20.580973</td>\n",
       "      <td>18.739861</td>\n",
       "      <td>53.656871</td>\n",
       "      <td>109.434371</td>\n",
       "      <td>42.703571</td>\n",
       "      <td>19.641667</td>\n",
       "      <td>19.314583</td>\n",
       "      <td>17.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1990-01-02</td>\n",
       "      <td>CAT</td>\n",
       "      <td>3.208576</td>\n",
       "      <td>7.359375</td>\n",
       "      <td>7.359375</td>\n",
       "      <td>7.234375</td>\n",
       "      <td>2910400.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.045713</td>\n",
       "      <td>7.750962</td>\n",
       "      <td>6.961538</td>\n",
       "      <td>47.910104</td>\n",
       "      <td>-16.224189</td>\n",
       "      <td>0.407785</td>\n",
       "      <td>7.323958</td>\n",
       "      <td>7.242448</td>\n",
       "      <td>17.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8187</th>\n",
       "      <td>2022-06-30</td>\n",
       "      <td>TRV</td>\n",
       "      <td>161.338013</td>\n",
       "      <td>169.130005</td>\n",
       "      <td>169.500000</td>\n",
       "      <td>164.610001</td>\n",
       "      <td>1336100.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-2.374215</td>\n",
       "      <td>180.224943</td>\n",
       "      <td>156.931054</td>\n",
       "      <td>44.639520</td>\n",
       "      <td>-44.912598</td>\n",
       "      <td>8.641378</td>\n",
       "      <td>170.339999</td>\n",
       "      <td>173.666833</td>\n",
       "      <td>29.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8187</th>\n",
       "      <td>2022-06-30</td>\n",
       "      <td>UNH</td>\n",
       "      <td>497.025818</td>\n",
       "      <td>513.630005</td>\n",
       "      <td>516.320007</td>\n",
       "      <td>513.549988</td>\n",
       "      <td>3568900.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.243815</td>\n",
       "      <td>522.217561</td>\n",
       "      <td>447.697438</td>\n",
       "      <td>54.941784</td>\n",
       "      <td>134.358930</td>\n",
       "      <td>26.659363</td>\n",
       "      <td>486.779667</td>\n",
       "      <td>500.980667</td>\n",
       "      <td>29.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8187</th>\n",
       "      <td>2022-06-30</td>\n",
       "      <td>VZ</td>\n",
       "      <td>43.047783</td>\n",
       "      <td>50.750000</td>\n",
       "      <td>51.220001</td>\n",
       "      <td>50.669998</td>\n",
       "      <td>18880500.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.170573</td>\n",
       "      <td>52.597046</td>\n",
       "      <td>48.530953</td>\n",
       "      <td>50.161884</td>\n",
       "      <td>21.317367</td>\n",
       "      <td>13.348827</td>\n",
       "      <td>50.415666</td>\n",
       "      <td>50.484500</td>\n",
       "      <td>29.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8187</th>\n",
       "      <td>2022-06-30</td>\n",
       "      <td>WBA</td>\n",
       "      <td>31.923557</td>\n",
       "      <td>37.900002</td>\n",
       "      <td>39.669998</td>\n",
       "      <td>39.520000</td>\n",
       "      <td>15611700.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.682825</td>\n",
       "      <td>44.006490</td>\n",
       "      <td>38.629509</td>\n",
       "      <td>39.126940</td>\n",
       "      <td>-160.885298</td>\n",
       "      <td>26.648574</td>\n",
       "      <td>41.693666</td>\n",
       "      <td>43.023000</td>\n",
       "      <td>29.42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8187</th>\n",
       "      <td>2022-06-30</td>\n",
       "      <td>WMT</td>\n",
       "      <td>39.213158</td>\n",
       "      <td>40.526669</td>\n",
       "      <td>40.686668</td>\n",
       "      <td>40.293331</td>\n",
       "      <td>19166100.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.903056</td>\n",
       "      <td>42.326373</td>\n",
       "      <td>38.975627</td>\n",
       "      <td>39.029583</td>\n",
       "      <td>-52.314867</td>\n",
       "      <td>19.015585</td>\n",
       "      <td>40.883111</td>\n",
       "      <td>46.010333</td>\n",
       "      <td>29.42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>204700 rows × 17 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Construct the environment\n",
    "\n",
    "Calculate and specify the parameters we need for constructing the environment."
   ],
   "id": "9ba8695833788a18"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:54:56.980483Z",
     "start_time": "2024-12-02T13:54:56.972551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "stock_dimension"
   ],
   "id": "2e8d0249126f35e4",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:54:59.790002Z",
     "start_time": "2024-12-02T13:54:59.786730Z"
    }
   },
   "cell_type": "code",
   "source": [
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "state_space"
   ],
   "id": "9da25790e0bbd8a8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-02T13:55:55.320624Z",
     "start_time": "2024-12-02T13:55:05.044422Z"
    }
   },
   "cell_type": "code",
   "source": [
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "num_stock_shares = [0] * stock_dimension\n",
    "\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100,\n",
    "    \"initial_amount\": 1000000,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-4,\n",
    "}\n",
    "\n",
    "e_train_gym = StockTradingEnv(df = train,**env_kwargs)"
   ],
   "id": "7a390cbeabd647d9",
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 17\u001B[0m\n\u001B[1;32m      2\u001B[0m num_stock_shares \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m*\u001B[39m stock_dimension\n\u001B[1;32m      4\u001B[0m env_kwargs \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhmax\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m100\u001B[39m,\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minitial_amount\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m1000000\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     14\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mreward_scaling\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m1e-4\u001B[39m,\n\u001B[1;32m     15\u001B[0m }\n\u001B[0;32m---> 17\u001B[0m e_train_gym \u001B[38;5;241m=\u001B[39m \u001B[43mStockTradingEnv\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdf\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mtrain\u001B[49m\u001B[43m,\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43menv_kwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRLExamples/.venv/lib/python3.10/site-packages/finrl/meta/env_stock_trading/env_stocktrading.py:65\u001B[0m, in \u001B[0;36mStockTradingEnv.__init__\u001B[0;34m(self, df, stock_dim, hmax, initial_amount, num_stock_shares, buy_cost_pct, sell_cost_pct, reward_scaling, state_space, action_space, tech_indicator_list, turbulence_threshold, risk_indicator_col, make_plots, print_verbosity, day, initial, previous_state, model_name, mode, iteration)\u001B[0m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservation_space \u001B[38;5;241m=\u001B[39m spaces\u001B[38;5;241m.\u001B[39mBox(\n\u001B[1;32m     62\u001B[0m     low\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39mnp\u001B[38;5;241m.\u001B[39minf, high\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39minf, shape\u001B[38;5;241m=\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate_space,)\n\u001B[1;32m     63\u001B[0m )\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdf\u001B[38;5;241m.\u001B[39mloc[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mday, :]\n\u001B[0;32m---> 65\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mterminal \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmake_plots \u001B[38;5;241m=\u001B[39m make_plots\n\u001B[1;32m     67\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_verbosity \u001B[38;5;241m=\u001B[39m print_verbosity\n",
      "File \u001B[0;32m~/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRLExamples/.venv/lib/python3.10/site-packages/finrl/meta/env_stock_trading/env_stocktrading.py:65\u001B[0m, in \u001B[0;36mStockTradingEnv.__init__\u001B[0;34m(self, df, stock_dim, hmax, initial_amount, num_stock_shares, buy_cost_pct, sell_cost_pct, reward_scaling, state_space, action_space, tech_indicator_list, turbulence_threshold, risk_indicator_col, make_plots, print_verbosity, day, initial, previous_state, model_name, mode, iteration)\u001B[0m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mobservation_space \u001B[38;5;241m=\u001B[39m spaces\u001B[38;5;241m.\u001B[39mBox(\n\u001B[1;32m     62\u001B[0m     low\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39mnp\u001B[38;5;241m.\u001B[39minf, high\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39minf, shape\u001B[38;5;241m=\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate_space,)\n\u001B[1;32m     63\u001B[0m )\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdata \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdf\u001B[38;5;241m.\u001B[39mloc[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mday, :]\n\u001B[0;32m---> 65\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mterminal \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     66\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmake_plots \u001B[38;5;241m=\u001B[39m make_plots\n\u001B[1;32m     67\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprint_verbosity \u001B[38;5;241m=\u001B[39m print_verbosity\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_310_64.pyx:1187\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_310_64.SafeCallWrapper.__call__\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_310_64.pyx:627\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_310_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_310_64.pyx:1103\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_310_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_310_64.pyx:1065\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_310_64.PyDBFrame.trace_dispatch\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m_pydevd_bundle/pydevd_cython_darwin_310_64.pyx:585\u001B[0m, in \u001B[0;36m_pydevd_bundle.pydevd_cython_darwin_310_64.PyDBFrame.do_wait_suspend\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py:1201\u001B[0m, in \u001B[0;36mPyDB.do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, send_suspend_message, is_unhandled_exception)\u001B[0m\n\u001B[1;32m   1198\u001B[0m         from_this_thread\u001B[38;5;241m.\u001B[39mappend(frame_id)\n\u001B[1;32m   1200\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_threads_suspended_single_notification\u001B[38;5;241m.\u001B[39mnotify_thread_suspended(thread_id, stop_reason):\n\u001B[0;32m-> 1201\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_do_wait_suspend\u001B[49m\u001B[43m(\u001B[49m\u001B[43mthread\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mframe\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mevent\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43marg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msuspend_type\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mfrom_this_thread\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Applications/PyCharm.app/Contents/plugins/python/helpers/pydev/pydevd.py:1216\u001B[0m, in \u001B[0;36mPyDB._do_wait_suspend\u001B[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread)\u001B[0m\n\u001B[1;32m   1213\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_mpl_hook()\n\u001B[1;32m   1215\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprocess_internal_commands()\n\u001B[0;32m-> 1216\u001B[0m         \u001B[43mtime\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msleep\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m0.01\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1218\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcancel_async_evaluation(get_current_thread_id(thread), \u001B[38;5;28mstr\u001B[39m(\u001B[38;5;28mid\u001B[39m(frame)))\n\u001B[1;32m   1220\u001B[0m \u001B[38;5;66;03m# process any stepping instructions\u001B[39;00m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Environment for training",
   "id": "867a8f06a4bd8d05"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T13:06:33.896720Z",
     "start_time": "2024-12-01T13:06:33.884638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(type(env_train))"
   ],
   "id": "f0e4f61599c8bee1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Part 3: Train DRL Agents\n",
    "* Here, the DRL algorithms are from **[Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/)**. It's a library that implemented popular DRL algorithms using pytorch, succeeding to its old version: Stable Baselines.\n",
    "* Users are also encouraged to try **[ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL)** and **[Ray RLlib](https://github.com/ray-project/ray)**."
   ],
   "id": "16ca393a156c4352"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T14:59:17.106363Z",
     "start_time": "2024-12-01T14:59:17.103635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent = DRLAgent(env_train)\n",
    "\n",
    "if_using_a2c = True\n",
    "if_using_ddpg = True\n",
    "if_using_ppo = True\n",
    "if_using_td3 = True\n",
    "if_using_sac = True\n"
   ],
   "id": "ac423cf782034bfa",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Agent Training: 5 algorithms (A2C, DDPG, PPO, TD3, SAC)",
   "id": "95185f80f063b16c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Agent 1: A2C",
   "id": "3e230ad7c22f5e1a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T13:06:43.521051Z",
     "start_time": "2024-12-01T13:06:42.877768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent = DRLAgent(env_train)\n",
    "model_a2c = agent.get_model('a2c')\n",
    "\n",
    "if if_using_a2c:\n",
    "    tmp_path = RESULTS_DIR + '/a2c'\n",
    "    new_logger_a2c = configure(tmp_path, [\"stdout\",\"csv\", \"tensorboard\"])\n",
    "    model_a2c.set_logger(new_logger_a2c)"
   ],
   "id": "9454597d91749af5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to results/a2c\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T13:16:05.546633Z",
     "start_time": "2024-12-01T13:06:51.749827Z"
    }
   },
   "cell_type": "code",
   "source": "trained_a2c = agent.train_model(model=model_a2c, tb_log_name='a2c', total_timesteps=50000) if if_using_a2c else None",
   "id": "78e10179c8bb9a4c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRLExamples/.venv/lib/python3.10/site-packages/finrl/meta/env_stock_trading/env_stocktrading.py:179: RuntimeWarning: divide by zero encountered in scalar floor_divide\n",
      "  available_amount = self.state[0] // (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 86          |\n",
      "|    iterations         | 100         |\n",
      "|    time_elapsed       | 5           |\n",
      "|    total_timesteps    | 500         |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -35.8       |\n",
      "|    explained_variance | -0.111      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 99          |\n",
      "|    policy_loss        | 54.2        |\n",
      "|    reward             | -0.03271399 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 2.84        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 87        |\n",
      "|    iterations         | 200       |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 1000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -35.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 199       |\n",
      "|    policy_loss        | 54.9      |\n",
      "|    reward             | 1.5392499 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 2.59      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 88       |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 1500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -35.9    |\n",
      "|    explained_variance | -0.156   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -15.2    |\n",
      "|    reward             | 2.931699 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 0.949    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 88       |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 22       |\n",
      "|    total_timesteps    | 2000     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -35.8    |\n",
      "|    explained_variance | -0.00537 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 215      |\n",
      "|    reward             | 6.058682 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 85       |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 88       |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 28       |\n",
      "|    total_timesteps    | 2500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -35.8    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -102     |\n",
      "|    reward             | 8.34229  |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 40.1     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 88        |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 33        |\n",
      "|    total_timesteps    | 3000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -35.8     |\n",
      "|    explained_variance | -0.0146   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | 80.6      |\n",
      "|    reward             | 2.2408829 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 22.2      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 88       |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 39       |\n",
      "|    total_timesteps    | 3500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -35.8    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 118      |\n",
      "|    reward             | 5.35952  |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 14.3     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 88        |\n",
      "|    iterations         | 800       |\n",
      "|    time_elapsed       | 45        |\n",
      "|    total_timesteps    | 4000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -35.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 799       |\n",
      "|    policy_loss        | -103      |\n",
      "|    reward             | 1.5220863 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 22.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 88        |\n",
      "|    iterations         | 900       |\n",
      "|    time_elapsed       | 50        |\n",
      "|    total_timesteps    | 4500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -35.7     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 899       |\n",
      "|    policy_loss        | 227       |\n",
      "|    reward             | 4.5451045 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 58.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 88        |\n",
      "|    iterations         | 1000      |\n",
      "|    time_elapsed       | 56        |\n",
      "|    total_timesteps    | 5000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -35.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 999       |\n",
      "|    policy_loss        | 27.8      |\n",
      "|    reward             | 1.6994456 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 35.1      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 88         |\n",
      "|    iterations         | 1100       |\n",
      "|    time_elapsed       | 61         |\n",
      "|    total_timesteps    | 5500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -35.8      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1099       |\n",
      "|    policy_loss        | -421       |\n",
      "|    reward             | 0.30483845 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 157        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 88         |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 67         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -35.8      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | 628        |\n",
      "|    reward             | -3.7427435 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 375        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 1300      |\n",
      "|    time_elapsed       | 73        |\n",
      "|    total_timesteps    | 6500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -35.9     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1299      |\n",
      "|    policy_loss        | 1.4e+03   |\n",
      "|    reward             | 12.419367 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.5e+03   |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 1400      |\n",
      "|    time_elapsed       | 78        |\n",
      "|    total_timesteps    | 7000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -35.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1399      |\n",
      "|    policy_loss        | 293       |\n",
      "|    reward             | 5.1690207 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 116       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 1500       |\n",
      "|    time_elapsed       | 84         |\n",
      "|    total_timesteps    | 7500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -35.9      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1499       |\n",
      "|    policy_loss        | -525       |\n",
      "|    reward             | -19.572449 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 479        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 1600       |\n",
      "|    time_elapsed       | 89         |\n",
      "|    total_timesteps    | 8000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -35.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1599       |\n",
      "|    policy_loss        | 1.41e+03   |\n",
      "|    reward             | -27.639004 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 1.52e+03   |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRLExamples/.venv/lib/python3.10/site-packages/finrl/meta/env_stock_trading/env_stocktrading.py:179: RuntimeWarning: divide by zero encountered in scalar floor_divide\n",
      "  available_amount = self.state[0] // (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 95        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36       |\n",
      "|    explained_variance | 0.279     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | -53       |\n",
      "|    reward             | 1.7516812 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 2.57      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 101        |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -36.1      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 8.63       |\n",
      "|    reward             | -1.1590841 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 0.221      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 1900      |\n",
      "|    time_elapsed       | 106       |\n",
      "|    total_timesteps    | 9500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1899      |\n",
      "|    policy_loss        | -29.3     |\n",
      "|    reward             | 1.9267263 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.11      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 89          |\n",
      "|    iterations         | 2000        |\n",
      "|    time_elapsed       | 112         |\n",
      "|    total_timesteps    | 10000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -36.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 1999        |\n",
      "|    policy_loss        | -159        |\n",
      "|    reward             | -0.25791234 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 26.9        |\n",
      "---------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 89       |\n",
      "|    iterations         | 2100     |\n",
      "|    time_elapsed       | 117      |\n",
      "|    total_timesteps    | 10500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -36.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2099     |\n",
      "|    policy_loss        | 165      |\n",
      "|    reward             | -7.48224 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 34.1     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 2200      |\n",
      "|    time_elapsed       | 123       |\n",
      "|    total_timesteps    | 11000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.1     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2199      |\n",
      "|    policy_loss        | 16        |\n",
      "|    reward             | 1.8564495 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 4.96      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 89          |\n",
      "|    iterations         | 2300        |\n",
      "|    time_elapsed       | 128         |\n",
      "|    total_timesteps    | 11500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -36.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2299        |\n",
      "|    policy_loss        | -84.5       |\n",
      "|    reward             | -0.30422914 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 8           |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 89          |\n",
      "|    iterations         | 2400        |\n",
      "|    time_elapsed       | 134         |\n",
      "|    total_timesteps    | 12000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -36.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2399        |\n",
      "|    policy_loss        | 76.7        |\n",
      "|    reward             | -0.94030344 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 18.1        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 2500      |\n",
      "|    time_elapsed       | 139       |\n",
      "|    total_timesteps    | 12500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2499      |\n",
      "|    policy_loss        | 300       |\n",
      "|    reward             | -4.814532 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 76.5      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 2600       |\n",
      "|    time_elapsed       | 145        |\n",
      "|    total_timesteps    | 13000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -36.1      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2599       |\n",
      "|    policy_loss        | 65.4       |\n",
      "|    reward             | -2.3638632 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 12         |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 2700      |\n",
      "|    time_elapsed       | 151       |\n",
      "|    total_timesteps    | 13500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.1     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2699      |\n",
      "|    policy_loss        | 100       |\n",
      "|    reward             | -9.633465 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 20.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 2800      |\n",
      "|    time_elapsed       | 156       |\n",
      "|    total_timesteps    | 14000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.1     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2799      |\n",
      "|    policy_loss        | -139      |\n",
      "|    reward             | 4.318742  |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 17.7      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 89       |\n",
      "|    iterations         | 2900     |\n",
      "|    time_elapsed       | 162      |\n",
      "|    total_timesteps    | 14500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -36.1    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2899     |\n",
      "|    policy_loss        | 212      |\n",
      "|    reward             | 9.993749 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 50.8     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 3000       |\n",
      "|    time_elapsed       | 167        |\n",
      "|    total_timesteps    | 15000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -36.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2999       |\n",
      "|    policy_loss        | 146        |\n",
      "|    reward             | 0.56903714 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 22.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 3100      |\n",
      "|    time_elapsed       | 173       |\n",
      "|    total_timesteps    | 15500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.1     |\n",
      "|    explained_variance | -2.38e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3099      |\n",
      "|    policy_loss        | 721       |\n",
      "|    reward             | 23.372185 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 1.27e+03  |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 3200       |\n",
      "|    time_elapsed       | 178        |\n",
      "|    total_timesteps    | 16000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -36.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3199       |\n",
      "|    policy_loss        | -304       |\n",
      "|    reward             | -10.895125 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 272        |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRLExamples/.venv/lib/python3.10/site-packages/finrl/meta/env_stock_trading/env_stocktrading.py:179: RuntimeWarning: divide by zero encountered in scalar floor_divide\n",
      "  available_amount = self.state[0] // (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 3300       |\n",
      "|    time_elapsed       | 184        |\n",
      "|    total_timesteps    | 16500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -36.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3299       |\n",
      "|    policy_loss        | -33        |\n",
      "|    reward             | 0.96497303 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.07       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 3400       |\n",
      "|    time_elapsed       | 190        |\n",
      "|    total_timesteps    | 17000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -36.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3399       |\n",
      "|    policy_loss        | -23.2      |\n",
      "|    reward             | -1.1265731 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.76       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 3500       |\n",
      "|    time_elapsed       | 195        |\n",
      "|    total_timesteps    | 17500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -36.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3499       |\n",
      "|    policy_loss        | 100        |\n",
      "|    reward             | 0.39722034 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 9.03       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 3600      |\n",
      "|    time_elapsed       | 201       |\n",
      "|    total_timesteps    | 18000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3599      |\n",
      "|    policy_loss        | 94.3      |\n",
      "|    reward             | -0.748232 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 12.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 3700       |\n",
      "|    time_elapsed       | 206        |\n",
      "|    total_timesteps    | 18500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -36.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3699       |\n",
      "|    policy_loss        | -169       |\n",
      "|    reward             | -0.9690422 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 25.2       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 3800       |\n",
      "|    time_elapsed       | 212        |\n",
      "|    total_timesteps    | 19000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -36.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3799       |\n",
      "|    policy_loss        | 83.7       |\n",
      "|    reward             | -0.7592886 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 9.37       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 3900       |\n",
      "|    time_elapsed       | 218        |\n",
      "|    total_timesteps    | 19500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -36.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3899       |\n",
      "|    policy_loss        | 234        |\n",
      "|    reward             | -5.0495577 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 47.3       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 4000       |\n",
      "|    time_elapsed       | 223        |\n",
      "|    total_timesteps    | 20000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -36.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3999       |\n",
      "|    policy_loss        | -16.2      |\n",
      "|    reward             | -2.6083746 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 2.03       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 4100       |\n",
      "|    time_elapsed       | 229        |\n",
      "|    total_timesteps    | 20500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -36.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4099       |\n",
      "|    policy_loss        | 94.7       |\n",
      "|    reward             | 0.25939694 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 7.15       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 4200       |\n",
      "|    time_elapsed       | 234        |\n",
      "|    total_timesteps    | 21000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -36.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4199       |\n",
      "|    policy_loss        | 274        |\n",
      "|    reward             | 0.33044067 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 117        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 89       |\n",
      "|    iterations         | 4300     |\n",
      "|    time_elapsed       | 240      |\n",
      "|    total_timesteps    | 21500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -36.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4299     |\n",
      "|    policy_loss        | 364      |\n",
      "|    reward             | 4.261346 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 142      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 4400      |\n",
      "|    time_elapsed       | 245       |\n",
      "|    total_timesteps    | 22000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4399      |\n",
      "|    policy_loss        | 158       |\n",
      "|    reward             | 1.8162273 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 30.9      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 4500      |\n",
      "|    time_elapsed       | 251       |\n",
      "|    total_timesteps    | 22500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.2     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4499      |\n",
      "|    policy_loss        | -286      |\n",
      "|    reward             | 0.5768172 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 163       |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 4600       |\n",
      "|    time_elapsed       | 256        |\n",
      "|    total_timesteps    | 23000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -36.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4599       |\n",
      "|    policy_loss        | -187       |\n",
      "|    reward             | 0.26038933 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 39.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 4700       |\n",
      "|    time_elapsed       | 262        |\n",
      "|    total_timesteps    | 23500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -36.1      |\n",
      "|    explained_variance | -2.38e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4699       |\n",
      "|    policy_loss        | -2.14e+03  |\n",
      "|    reward             | -1.2851983 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 4.25e+03   |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 4800      |\n",
      "|    time_elapsed       | 267       |\n",
      "|    total_timesteps    | 24000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4799      |\n",
      "|    policy_loss        | 1.55e+03  |\n",
      "|    reward             | 41.897427 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 1.42e+04  |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 4900      |\n",
      "|    time_elapsed       | 273       |\n",
      "|    total_timesteps    | 24500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4899      |\n",
      "|    policy_loss        | 488       |\n",
      "|    reward             | 1.7680578 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 285       |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRLExamples/.venv/lib/python3.10/site-packages/finrl/meta/env_stock_trading/env_stocktrading.py:179: RuntimeWarning: divide by zero encountered in scalar floor_divide\n",
      "  available_amount = self.state[0] // (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 89          |\n",
      "|    iterations         | 5000        |\n",
      "|    time_elapsed       | 278         |\n",
      "|    total_timesteps    | 25000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -36.1       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4999        |\n",
      "|    policy_loss        | -106        |\n",
      "|    reward             | -0.52426416 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 9.97        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 5100       |\n",
      "|    time_elapsed       | 284        |\n",
      "|    total_timesteps    | 25500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -36.2      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5099       |\n",
      "|    policy_loss        | 28         |\n",
      "|    reward             | -0.6917736 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.33       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 89       |\n",
      "|    iterations         | 5200     |\n",
      "|    time_elapsed       | 290      |\n",
      "|    total_timesteps    | 26000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -36.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5199     |\n",
      "|    policy_loss        | 12.1     |\n",
      "|    reward             | 2.552751 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 1.62     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 5300      |\n",
      "|    time_elapsed       | 296       |\n",
      "|    total_timesteps    | 26500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.3     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5299      |\n",
      "|    policy_loss        | 106       |\n",
      "|    reward             | -3.955608 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 24.9      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 5400      |\n",
      "|    time_elapsed       | 302       |\n",
      "|    total_timesteps    | 27000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5399      |\n",
      "|    policy_loss        | -30       |\n",
      "|    reward             | 1.9909228 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 5.12      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 5500      |\n",
      "|    time_elapsed       | 307       |\n",
      "|    total_timesteps    | 27500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5499      |\n",
      "|    policy_loss        | -103      |\n",
      "|    reward             | -4.495074 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 16.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 5600      |\n",
      "|    time_elapsed       | 313       |\n",
      "|    total_timesteps    | 28000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.2     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5599      |\n",
      "|    policy_loss        | 14.4      |\n",
      "|    reward             | 1.1945453 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 6.92      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 5700      |\n",
      "|    time_elapsed       | 318       |\n",
      "|    total_timesteps    | 28500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5699      |\n",
      "|    policy_loss        | 43.2      |\n",
      "|    reward             | 5.9845505 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 7.67      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 5800      |\n",
      "|    time_elapsed       | 324       |\n",
      "|    total_timesteps    | 29000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5799      |\n",
      "|    policy_loss        | 172       |\n",
      "|    reward             | 1.9518695 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 46.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 5900       |\n",
      "|    time_elapsed       | 329        |\n",
      "|    total_timesteps    | 29500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -36.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5899       |\n",
      "|    policy_loss        | 88         |\n",
      "|    reward             | -2.7880409 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 9.51       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 6000       |\n",
      "|    time_elapsed       | 335        |\n",
      "|    total_timesteps    | 30000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -36.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5999       |\n",
      "|    policy_loss        | -396       |\n",
      "|    reward             | -0.2296459 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 130        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 6100      |\n",
      "|    time_elapsed       | 340       |\n",
      "|    total_timesteps    | 30500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6099      |\n",
      "|    policy_loss        | 118       |\n",
      "|    reward             | 1.9946421 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 45.8      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 6200       |\n",
      "|    time_elapsed       | 345        |\n",
      "|    total_timesteps    | 31000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -36        |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6199       |\n",
      "|    policy_loss        | 640        |\n",
      "|    reward             | -1.5669074 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 414        |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 89       |\n",
      "|    iterations         | 6300     |\n",
      "|    time_elapsed       | 351      |\n",
      "|    total_timesteps    | 31500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -36.1    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6299     |\n",
      "|    policy_loss        | 197      |\n",
      "|    reward             | 8.682122 |\n",
      "|    std                | 1.03     |\n",
      "|    value_loss         | 67.2     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 6400      |\n",
      "|    time_elapsed       | 356       |\n",
      "|    total_timesteps    | 32000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.1     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6399      |\n",
      "|    policy_loss        | -226      |\n",
      "|    reward             | -7.561189 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 227       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 6500      |\n",
      "|    time_elapsed       | 362       |\n",
      "|    total_timesteps    | 32500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.1     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6499      |\n",
      "|    policy_loss        | 695       |\n",
      "|    reward             | 20.174906 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 613       |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRLExamples/.venv/lib/python3.10/site-packages/finrl/meta/env_stock_trading/env_stocktrading.py:179: RuntimeWarning: divide by zero encountered in scalar floor_divide\n",
      "  available_amount = self.state[0] // (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 89          |\n",
      "|    iterations         | 6600        |\n",
      "|    time_elapsed       | 367         |\n",
      "|    total_timesteps    | 33000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -36.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6599        |\n",
      "|    policy_loss        | -55.7       |\n",
      "|    reward             | -0.42990914 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 3.03        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 6700      |\n",
      "|    time_elapsed       | 373       |\n",
      "|    total_timesteps    | 33500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6699      |\n",
      "|    policy_loss        | 13.9      |\n",
      "|    reward             | 0.8596568 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 0.486     |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 6800      |\n",
      "|    time_elapsed       | 378       |\n",
      "|    total_timesteps    | 34000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.2     |\n",
      "|    explained_variance | -0.0915   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6799      |\n",
      "|    policy_loss        | 55        |\n",
      "|    reward             | 1.3377376 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 2.94      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 6900       |\n",
      "|    time_elapsed       | 384        |\n",
      "|    total_timesteps    | 34500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -36.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6899       |\n",
      "|    policy_loss        | 305        |\n",
      "|    reward             | 0.32739097 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 86.5       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 7000      |\n",
      "|    time_elapsed       | 389       |\n",
      "|    total_timesteps    | 35000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.2     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 6999      |\n",
      "|    policy_loss        | -384      |\n",
      "|    reward             | 1.7439499 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 126       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 7100      |\n",
      "|    time_elapsed       | 395       |\n",
      "|    total_timesteps    | 35500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.1     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7099      |\n",
      "|    policy_loss        | -29.4     |\n",
      "|    reward             | 5.5898476 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 12.6      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 7200      |\n",
      "|    time_elapsed       | 400       |\n",
      "|    total_timesteps    | 36000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.2     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7199      |\n",
      "|    policy_loss        | 137       |\n",
      "|    reward             | 7.7458324 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 67.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 7300      |\n",
      "|    time_elapsed       | 406       |\n",
      "|    total_timesteps    | 36500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7299      |\n",
      "|    policy_loss        | 310       |\n",
      "|    reward             | 1.6923591 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 83.4      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 7400       |\n",
      "|    time_elapsed       | 411        |\n",
      "|    total_timesteps    | 37000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -36.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7399       |\n",
      "|    policy_loss        | -97.5      |\n",
      "|    reward             | 0.44378012 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 10.6       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 7500      |\n",
      "|    time_elapsed       | 417       |\n",
      "|    total_timesteps    | 37500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7499      |\n",
      "|    policy_loss        | -627      |\n",
      "|    reward             | 5.5473957 |\n",
      "|    std                | 1.03      |\n",
      "|    value_loss         | 341       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 7600      |\n",
      "|    time_elapsed       | 422       |\n",
      "|    total_timesteps    | 38000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7599      |\n",
      "|    policy_loss        | -356      |\n",
      "|    reward             | -8.162478 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 190       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 89        |\n",
      "|    iterations         | 7700      |\n",
      "|    time_elapsed       | 427       |\n",
      "|    total_timesteps    | 38500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7699      |\n",
      "|    policy_loss        | 217       |\n",
      "|    reward             | 1.0715522 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 39.6      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 89         |\n",
      "|    iterations         | 7800       |\n",
      "|    time_elapsed       | 433        |\n",
      "|    total_timesteps    | 39000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -36.4      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7799       |\n",
      "|    policy_loss        | 95.1       |\n",
      "|    reward             | -19.365074 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 43.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 90        |\n",
      "|    iterations         | 7900      |\n",
      "|    time_elapsed       | 438       |\n",
      "|    total_timesteps    | 39500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7899      |\n",
      "|    policy_loss        | 9.14      |\n",
      "|    reward             | 0.6155612 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 25.2      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 90       |\n",
      "|    iterations         | 8000     |\n",
      "|    time_elapsed       | 444      |\n",
      "|    total_timesteps    | 40000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -36.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 7999     |\n",
      "|    policy_loss        | -629     |\n",
      "|    reward             | 4.499637 |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 379      |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 90        |\n",
      "|    iterations         | 8100      |\n",
      "|    time_elapsed       | 449       |\n",
      "|    total_timesteps    | 40500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8099      |\n",
      "|    policy_loss        | -922      |\n",
      "|    reward             | 11.974303 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 768       |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRLExamples/.venv/lib/python3.10/site-packages/finrl/meta/env_stock_trading/env_stocktrading.py:179: RuntimeWarning: divide by zero encountered in scalar floor_divide\n",
      "  available_amount = self.state[0] // (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 90         |\n",
      "|    iterations         | 8200       |\n",
      "|    time_elapsed       | 455        |\n",
      "|    total_timesteps    | 41000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -36.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8199       |\n",
      "|    policy_loss        | 8.9        |\n",
      "|    reward             | 0.17636506 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 0.139      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 90         |\n",
      "|    iterations         | 8300       |\n",
      "|    time_elapsed       | 460        |\n",
      "|    total_timesteps    | 41500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -36.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8299       |\n",
      "|    policy_loss        | -75.7      |\n",
      "|    reward             | 0.52084005 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 8.04       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 90         |\n",
      "|    iterations         | 8400       |\n",
      "|    time_elapsed       | 466        |\n",
      "|    total_timesteps    | 42000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -36.4      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8399       |\n",
      "|    policy_loss        | -46.9      |\n",
      "|    reward             | -0.8239124 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 2.99       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 90         |\n",
      "|    iterations         | 8500       |\n",
      "|    time_elapsed       | 471        |\n",
      "|    total_timesteps    | 42500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -36.4      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8499       |\n",
      "|    policy_loss        | 45.3       |\n",
      "|    reward             | 0.52504176 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 6.75       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 90         |\n",
      "|    iterations         | 8600       |\n",
      "|    time_elapsed       | 477        |\n",
      "|    total_timesteps    | 43000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -36.4      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8599       |\n",
      "|    policy_loss        | 381        |\n",
      "|    reward             | -1.1139145 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 108        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 90         |\n",
      "|    iterations         | 8700       |\n",
      "|    time_elapsed       | 482        |\n",
      "|    total_timesteps    | 43500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -36.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8699       |\n",
      "|    policy_loss        | -482       |\n",
      "|    reward             | -1.9167948 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 204        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 90         |\n",
      "|    iterations         | 8800       |\n",
      "|    time_elapsed       | 488        |\n",
      "|    total_timesteps    | 44000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -36.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8799       |\n",
      "|    policy_loss        | 193        |\n",
      "|    reward             | -4.3916626 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 47.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 90        |\n",
      "|    iterations         | 8900      |\n",
      "|    time_elapsed       | 493       |\n",
      "|    total_timesteps    | 44500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8899      |\n",
      "|    policy_loss        | 108       |\n",
      "|    reward             | 3.6901987 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 11.9      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 90        |\n",
      "|    iterations         | 9000      |\n",
      "|    time_elapsed       | 499       |\n",
      "|    total_timesteps    | 45000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8999      |\n",
      "|    policy_loss        | 237       |\n",
      "|    reward             | 0.5490151 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 49.2      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 90        |\n",
      "|    iterations         | 9100      |\n",
      "|    time_elapsed       | 504       |\n",
      "|    total_timesteps    | 45500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9099      |\n",
      "|    policy_loss        | 193       |\n",
      "|    reward             | 1.1223677 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 227       |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 90        |\n",
      "|    iterations         | 9200      |\n",
      "|    time_elapsed       | 509       |\n",
      "|    total_timesteps    | 46000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.6     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9199      |\n",
      "|    policy_loss        | -153      |\n",
      "|    reward             | 12.934981 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 25.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 90        |\n",
      "|    iterations         | 9300      |\n",
      "|    time_elapsed       | 515       |\n",
      "|    total_timesteps    | 46500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9299      |\n",
      "|    policy_loss        | -97.8     |\n",
      "|    reward             | 2.6837807 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 13.9      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 90        |\n",
      "|    iterations         | 9400      |\n",
      "|    time_elapsed       | 520       |\n",
      "|    total_timesteps    | 47000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.6     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9399      |\n",
      "|    policy_loss        | 53.4      |\n",
      "|    reward             | 1.6063226 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 9.42      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 90         |\n",
      "|    iterations         | 9500       |\n",
      "|    time_elapsed       | 526        |\n",
      "|    total_timesteps    | 47500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -36.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9499       |\n",
      "|    policy_loss        | -820       |\n",
      "|    reward             | 0.68003786 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 510        |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 90         |\n",
      "|    iterations         | 9600       |\n",
      "|    time_elapsed       | 531        |\n",
      "|    total_timesteps    | 48000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -36.5      |\n",
      "|    explained_variance | 5.96e-08   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9599       |\n",
      "|    policy_loss        | 155        |\n",
      "|    reward             | -3.8873568 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 25.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 90         |\n",
      "|    iterations         | 9700       |\n",
      "|    time_elapsed       | 537        |\n",
      "|    total_timesteps    | 48500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -36.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9699       |\n",
      "|    policy_loss        | 42.8       |\n",
      "|    reward             | -14.708364 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 7.2        |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 90        |\n",
      "|    iterations         | 9800      |\n",
      "|    time_elapsed       | 542       |\n",
      "|    total_timesteps    | 49000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.5     |\n",
      "|    explained_variance | -6.66e-05 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9799      |\n",
      "|    policy_loss        | -824      |\n",
      "|    reward             | 5.092994  |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 504       |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRLExamples/.venv/lib/python3.10/site-packages/finrl/meta/env_stock_trading/env_stocktrading.py:179: RuntimeWarning: divide by zero encountered in scalar floor_divide\n",
      "  available_amount = self.state[0] // (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 90         |\n",
      "|    iterations         | 9900       |\n",
      "|    time_elapsed       | 548        |\n",
      "|    total_timesteps    | 49500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -36.5      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9899       |\n",
      "|    policy_loss        | -1.33      |\n",
      "|    reward             | 0.42677584 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 0.327      |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 90        |\n",
      "|    iterations         | 10000     |\n",
      "|    time_elapsed       | 553       |\n",
      "|    total_timesteps    | 50000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -36.5     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9999      |\n",
      "|    policy_loss        | -47.2     |\n",
      "|    reward             | 0.6265745 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 1.99      |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T13:17:23.334152Z",
     "start_time": "2024-12-01T13:17:23.323065Z"
    }
   },
   "cell_type": "code",
   "source": "trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None",
   "id": "9c73e40e0491b0c1",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Agent 2: DDPG",
   "id": "796a5edc5259b2ad"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T13:17:39.984960Z",
     "start_time": "2024-12-01T13:17:39.970438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_ddpg = agent.get_model('ddpg')\n",
    "\n",
    "if if_using_ddpg:\n",
    "    tmp_path = RESULTS_DIR + '/ddpg'\n",
    "    new_logger_ddpg = configure(tmp_path, [\"stdout\",\"csv\",\"tensorboard\"])\n",
    "    model_ddpg.set_logger(new_logger_ddpg)"
   ],
   "id": "67cd1b2abaa084c7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
      "Using cpu device\n",
      "Logging to results/ddpg\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T13:31:41.788595Z",
     "start_time": "2024-12-01T13:17:52.445776Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trained_ddpg = agent.train_model(model=model_ddpg,\n",
    "                                 tb_log_name='ddpg',\n",
    "                                 total_timesteps=50000) if if_using_ddpg else None"
   ],
   "id": "496e8d21a0d2b1e7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRLExamples/.venv/lib/python3.10/site-packages/finrl/meta/env_stock_trading/env_stocktrading.py:179: RuntimeWarning: divide by zero encountered in scalar floor_divide\n",
      "  available_amount = self.state[0] // (\n",
      "/Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRLExamples/.venv/lib/python3.10/site-packages/finrl/meta/env_stock_trading/env_stocktrading.py:179: RuntimeWarning: divide by zero encountered in scalar floor_divide\n",
      "  available_amount = self.state[0] // (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 8187, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 15028863.25\n",
      "total_reward: 14028863.25\n",
      "total_cost: 999.00\n",
      "total_trades: 122805\n",
      "Sharpe: 0.580\n",
      "=================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRLExamples/.venv/lib/python3.10/site-packages/finrl/meta/env_stock_trading/env_stocktrading.py:179: RuntimeWarning: divide by zero encountered in scalar floor_divide\n",
      "  available_amount = self.state[0] // (\n",
      "/Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRLExamples/.venv/lib/python3.10/site-packages/finrl/meta/env_stock_trading/env_stocktrading.py:179: RuntimeWarning: divide by zero encountered in scalar floor_divide\n",
      "  available_amount = self.state[0] // (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 60         |\n",
      "|    time_elapsed    | 544        |\n",
      "|    total_timesteps | 32752      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 5.18       |\n",
      "|    critic_loss     | 30.8       |\n",
      "|    learning_rate   | 0.001      |\n",
      "|    n_updates       | 32651      |\n",
      "|    reward          | -14.222589 |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRLExamples/.venv/lib/python3.10/site-packages/finrl/meta/env_stock_trading/env_stocktrading.py:179: RuntimeWarning: divide by zero encountered in scalar floor_divide\n",
      "  available_amount = self.state[0] // (\n",
      "/Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRLExamples/.venv/lib/python3.10/site-packages/finrl/meta/env_stock_trading/env_stocktrading.py:179: RuntimeWarning: divide by zero encountered in scalar floor_divide\n",
      "  available_amount = self.state[0] // (\n",
      "/Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRLExamples/.venv/lib/python3.10/site-packages/finrl/meta/env_stock_trading/env_stocktrading.py:179: RuntimeWarning: divide by zero encountered in scalar floor_divide\n",
      "  available_amount = self.state[0] // (\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T13:38:19.224808Z",
     "start_time": "2024-12-01T13:38:19.210499Z"
    }
   },
   "cell_type": "code",
   "source": "trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None",
   "id": "1d7e9d0d8e2c7987",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Agent 3: PPO",
   "id": "1a8e59b1a1fa296b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T13:38:37.356885Z",
     "start_time": "2024-12-01T13:38:37.346114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 2048,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"learning_rate\": 0.00025,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "\n",
    "model_ppo = agent.get_model('ppo', model_kwargs=PPO_PARAMS)\n",
    "\n",
    "if if_using_ppo:\n",
    "    tmp_path = RESULTS_DIR + '/ppo'\n",
    "    new_logger_ppo = configure(tmp_path, [\"stdout\",\"csv\",\"tensorboard\"])\n",
    "    model_ppo.set_logger(new_logger_ppo)"
   ],
   "id": "a0fee6c6adccca51",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to results/ppo\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T14:42:55.321636Z",
     "start_time": "2024-12-01T14:33:31.157656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trained_ppo = agent.train_model(model=model_ppo,\n",
    "                                tb_log_name='ppo',\n",
    "                                total_timesteps=50000) if if_using_ppo else None"
   ],
   "id": "5cd9a1027065482a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRLExamples/.venv/lib/python3.10/site-packages/finrl/meta/env_stock_trading/env_stocktrading.py:179: RuntimeWarning: divide by zero encountered in scalar floor_divide\n",
      "  available_amount = self.state[0] // (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 1           |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 2048        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017176835 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -38         |\n",
      "|    explained_variance   | 0.00726     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 285         |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.00176    |\n",
      "|    reward               | -1.1960642  |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 601         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028025903 |\n",
      "|    clip_fraction        | 0.285       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -38         |\n",
      "|    explained_variance   | 0.499       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 8.67        |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | 0.000288    |\n",
      "|    reward               | 1.6290374   |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 15.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 89          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 68          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022091856 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -38.1       |\n",
      "|    explained_variance   | 0.0415      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 36.1        |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.00669    |\n",
      "|    reward               | 2.3846283   |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 106         |\n",
      "-----------------------------------------\n",
      "day: 8187, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 20312125.62\n",
      "total_reward: 19312125.62\n",
      "total_cost: 604331.88\n",
      "total_trades: 189440\n",
      "Sharpe: 0.589\n",
      "=================================\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 90           |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 90           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.014562528  |\n",
      "|    clip_fraction        | 0.153        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -38.1        |\n",
      "|    explained_variance   | -0.0402      |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 129          |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.00569     |\n",
      "|    reward               | -0.016561909 |\n",
      "|    std                  | 1.11         |\n",
      "|    value_loss           | 232          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRLExamples/.venv/lib/python3.10/site-packages/finrl/meta/env_stock_trading/env_stocktrading.py:179: RuntimeWarning: divide by zero encountered in scalar floor_divide\n",
      "  available_amount = self.state[0] // (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 113         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016996324 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -38.1       |\n",
      "|    explained_variance   | 0.00229     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 602         |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.00375    |\n",
      "|    reward               | -1.3652476  |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 1.54e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 136         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029459074 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -38.1       |\n",
      "|    explained_variance   | 0.174       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 4.98        |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.00653    |\n",
      "|    reward               | 0.8342952   |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 15.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 158         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022956088 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -38.1       |\n",
      "|    explained_variance   | 0.017       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 52.2        |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.00625    |\n",
      "|    reward               | 2.4886937   |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 111         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 90           |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 181          |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.013772625  |\n",
      "|    clip_fraction        | 0.185        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -38.1        |\n",
      "|    explained_variance   | -0.00222     |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 181          |\n",
      "|    n_updates            | 880          |\n",
      "|    policy_gradient_loss | 0.00203      |\n",
      "|    reward               | -0.036980547 |\n",
      "|    std                  | 1.11         |\n",
      "|    value_loss           | 322          |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRLExamples/.venv/lib/python3.10/site-packages/finrl/meta/env_stock_trading/env_stocktrading.py:179: RuntimeWarning: divide by zero encountered in scalar floor_divide\n",
      "  available_amount = self.state[0] // (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 203         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015288312 |\n",
      "|    clip_fraction        | 0.141       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -38.2       |\n",
      "|    explained_variance   | 0.00495     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.14e+03    |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.00941    |\n",
      "|    reward               | 1.4834479   |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 1.63e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 226         |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029635914 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -38.2       |\n",
      "|    explained_variance   | 0.195       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.94        |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.00671    |\n",
      "|    reward               | -0.75533956 |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 15.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 248         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014023664 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -38.2       |\n",
      "|    explained_variance   | 0.0346      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 79.2        |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.000552   |\n",
      "|    reward               | 5.6991186   |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 129         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 270         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026799105 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -38.2       |\n",
      "|    explained_variance   | -0.0328     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 116         |\n",
      "|    n_updates            | 920         |\n",
      "|    policy_gradient_loss | -0.00255    |\n",
      "|    reward               | -0.03622383 |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 245         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRLExamples/.venv/lib/python3.10/site-packages/finrl/meta/env_stock_trading/env_stocktrading.py:179: RuntimeWarning: divide by zero encountered in scalar floor_divide\n",
      "  available_amount = self.state[0] // (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 292         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014818619 |\n",
      "|    clip_fraction        | 0.136       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -38.3       |\n",
      "|    explained_variance   | 0.00326     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.34e+03    |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.00499    |\n",
      "|    reward               | 1.3736296   |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 2.49e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 314         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034964435 |\n",
      "|    clip_fraction        | 0.308       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -38.2       |\n",
      "|    explained_variance   | -0.0224     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 7.29        |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.0041     |\n",
      "|    reward               | -0.5934848  |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 15.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 336         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015422985 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -38.3       |\n",
      "|    explained_variance   | 0.0253      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 48.8        |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | 0.000524    |\n",
      "|    reward               | 3.4796767   |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 101         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 359         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022152364 |\n",
      "|    clip_fraction        | 0.18        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -38.3       |\n",
      "|    explained_variance   | -0.0286     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 93.5        |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.00391    |\n",
      "|    reward               | -0.10524339 |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 227         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRLExamples/.venv/lib/python3.10/site-packages/finrl/meta/env_stock_trading/env_stocktrading.py:179: RuntimeWarning: divide by zero encountered in scalar floor_divide\n",
      "  available_amount = self.state[0] // (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 381         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011697961 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -38.3       |\n",
      "|    explained_variance   | 0.00515     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.43e+03    |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.00541    |\n",
      "|    reward               | 1.6258563   |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 3.09e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 91         |\n",
      "|    iterations           | 18         |\n",
      "|    time_elapsed         | 404        |\n",
      "|    total_timesteps      | 36864      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03430905 |\n",
      "|    clip_fraction        | 0.305      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -38.3      |\n",
      "|    explained_variance   | 0.077      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 5.76       |\n",
      "|    n_updates            | 980        |\n",
      "|    policy_gradient_loss | -0.00788   |\n",
      "|    reward               | -6.961428  |\n",
      "|    std                  | 1.12       |\n",
      "|    value_loss           | 18         |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 427         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020481404 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -38.4       |\n",
      "|    explained_variance   | 0.0356      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 47.3        |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.000495   |\n",
      "|    reward               | -0.7004524  |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 146         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 449         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018562928 |\n",
      "|    clip_fraction        | 0.258       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -38.4       |\n",
      "|    explained_variance   | -0.0132     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 129         |\n",
      "|    n_updates            | 1000        |\n",
      "|    policy_gradient_loss | 0.00202     |\n",
      "|    reward               | 0.011163059 |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 378         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRLExamples/.venv/lib/python3.10/site-packages/finrl/meta/env_stock_trading/env_stocktrading.py:179: RuntimeWarning: divide by zero encountered in scalar floor_divide\n",
      "  available_amount = self.state[0] // (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 91          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 472         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015336868 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -38.5       |\n",
      "|    explained_variance   | 0.00225     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.53e+03    |\n",
      "|    n_updates            | 1010        |\n",
      "|    policy_gradient_loss | -0.00548    |\n",
      "|    reward               | 1.9652637   |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 4.54e+03    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 91         |\n",
      "|    iterations           | 22         |\n",
      "|    time_elapsed         | 495        |\n",
      "|    total_timesteps      | 45056      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02794404 |\n",
      "|    clip_fraction        | 0.245      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -38.6      |\n",
      "|    explained_variance   | 0.715      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 7.54       |\n",
      "|    n_updates            | 1020       |\n",
      "|    policy_gradient_loss | 0.000463   |\n",
      "|    reward               | 1.0980291  |\n",
      "|    std                  | 1.13       |\n",
      "|    value_loss           | 14.1       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 517         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020811867 |\n",
      "|    clip_fraction        | 0.251       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -38.6       |\n",
      "|    explained_variance   | -0.0107     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 69.8        |\n",
      "|    n_updates            | 1030        |\n",
      "|    policy_gradient_loss | -0.0019     |\n",
      "|    reward               | 3.6939588   |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 124         |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRLExamples/.venv/lib/python3.10/site-packages/finrl/meta/env_stock_trading/env_stocktrading.py:179: RuntimeWarning: divide by zero encountered in scalar floor_divide\n",
      "  available_amount = self.state[0] // (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 540         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021443706 |\n",
      "|    clip_fraction        | 0.245       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -38.6       |\n",
      "|    explained_variance   | -0.00324    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 99.2        |\n",
      "|    n_updates            | 1040        |\n",
      "|    policy_gradient_loss | -0.00438    |\n",
      "|    reward               | 0.08139352  |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 242         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 90          |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 563         |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008675712 |\n",
      "|    clip_fraction        | 0.0813      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -38.6       |\n",
      "|    explained_variance   | 0.00575     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 1.27e+03    |\n",
      "|    n_updates            | 1050        |\n",
      "|    policy_gradient_loss | -0.00319    |\n",
      "|    reward               | 0.5750359   |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 3e+03       |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T14:43:21.358493Z",
     "start_time": "2024-12-01T14:43:21.349218Z"
    }
   },
   "cell_type": "code",
   "source": "trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo\") if if_using_ppo else None",
   "id": "fc747c8b19361093",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Agent 4: TD3",
   "id": "ea60c6c4a9ce5450"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T14:44:05.352404Z",
     "start_time": "2024-12-01T14:44:05.337542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "TD3_PARAMS = {\"batch_size\": 100, \n",
    "              \"buffer_size\": 1000000, \n",
    "              \"learning_rate\": 0.001}\n",
    "\n",
    "model_td3 = agent.get_model(\"td3\",model_kwargs = TD3_PARAMS)\n",
    "\n",
    "if if_using_td3:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/td3'\n",
    "  new_logger_td3 = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_td3.set_logger(new_logger_td3)"
   ],
   "id": "a9d3e8a0cf9afab2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
      "Using cpu device\n",
      "Logging to results/td3\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T14:57:32.034398Z",
     "start_time": "2024-12-01T14:44:18.495462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trained_td3 = agent.train_model(model=model_td3, \n",
    "                             tb_log_name='td3',\n",
    "                             total_timesteps=50000) if if_using_td3 else None"
   ],
   "id": "2e18b81e9f94c0a7",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRLExamples/.venv/lib/python3.10/site-packages/finrl/meta/env_stock_trading/env_stocktrading.py:179: RuntimeWarning: divide by zero encountered in scalar floor_divide\n",
      "  available_amount = self.state[0] // (\n",
      "/Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRLExamples/.venv/lib/python3.10/site-packages/finrl/meta/env_stock_trading/env_stocktrading.py:179: RuntimeWarning: divide by zero encountered in scalar floor_divide\n",
      "  available_amount = self.state[0] // (\n",
      "/Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRLExamples/.venv/lib/python3.10/site-packages/finrl/meta/env_stock_trading/env_stocktrading.py:179: RuntimeWarning: divide by zero encountered in scalar floor_divide\n",
      "  available_amount = self.state[0] // (\n",
      "/Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRLExamples/.venv/lib/python3.10/site-packages/finrl/meta/env_stock_trading/env_stocktrading.py:179: RuntimeWarning: divide by zero encountered in scalar floor_divide\n",
      "  available_amount = self.state[0] // (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 8187, episode: 50\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 16626448.77\n",
      "total_reward: 15626448.77\n",
      "total_cost: 999.00\n",
      "total_trades: 114618\n",
      "Sharpe: 0.573\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 62         |\n",
      "|    time_elapsed    | 525        |\n",
      "|    total_timesteps | 32752      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | -11.7      |\n",
      "|    critic_loss     | 65.3       |\n",
      "|    learning_rate   | 0.001      |\n",
      "|    n_updates       | 32651      |\n",
      "|    reward          | -15.933864 |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRLExamples/.venv/lib/python3.10/site-packages/finrl/meta/env_stock_trading/env_stocktrading.py:179: RuntimeWarning: divide by zero encountered in scalar floor_divide\n",
      "  available_amount = self.state[0] // (\n",
      "/Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRLExamples/.venv/lib/python3.10/site-packages/finrl/meta/env_stock_trading/env_stocktrading.py:179: RuntimeWarning: divide by zero encountered in scalar floor_divide\n",
      "  available_amount = self.state[0] // (\n",
      "/Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRLExamples/.venv/lib/python3.10/site-packages/finrl/meta/env_stock_trading/env_stocktrading.py:179: RuntimeWarning: divide by zero encountered in scalar floor_divide\n",
      "  available_amount = self.state[0] // (\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T14:58:32.051118Z",
     "start_time": "2024-12-01T14:58:32.028514Z"
    }
   },
   "cell_type": "code",
   "source": "trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3\") if if_using_td3 else None",
   "id": "a4d5060c185f588a",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Agent 5: SAC",
   "id": "e31a12890edf70d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T14:59:24.239514Z",
     "start_time": "2024-12-01T14:59:24.224785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "SAC_PARAMS = {\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_size\": 100000,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"learning_starts\": 100,\n",
    "    \"ent_coef\": \"auto_0.1\",\n",
    "}\n",
    "\n",
    "model_sac = agent.get_model(\"sac\",model_kwargs = SAC_PARAMS)\n",
    "\n",
    "if if_using_sac:\n",
    "  # set up logger\n",
    "  tmp_path = RESULTS_DIR + '/sac'\n",
    "  new_logger_sac = configure(tmp_path, [\"stdout\", \"csv\", \"tensorboard\"])\n",
    "  # Set new logger\n",
    "  model_sac.set_logger(new_logger_sac)"
   ],
   "id": "d4c295370912ea95",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cpu device\n",
      "Logging to results/sac\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T15:20:19.305568Z",
     "start_time": "2024-12-01T14:59:37.659264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trained_sac = agent.train_model(model=model_sac, \n",
    "                             tb_log_name='sac',\n",
    "                             total_timesteps=70000) if if_using_sac else None"
   ],
   "id": "62ea9513be933ca2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRLExamples/.venv/lib/python3.10/site-packages/finrl/meta/env_stock_trading/env_stocktrading.py:179: RuntimeWarning: divide by zero encountered in scalar floor_divide\n",
      "  available_amount = self.state[0] // (\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 4          |\n",
      "|    fps             | 56         |\n",
      "|    time_elapsed    | 583        |\n",
      "|    total_timesteps | 32752      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 2.89e+04   |\n",
      "|    critic_loss     | 1.86e+03   |\n",
      "|    ent_coef        | 2.56       |\n",
      "|    ent_coef_loss   | -284       |\n",
      "|    learning_rate   | 0.0001     |\n",
      "|    n_updates       | 32651      |\n",
      "|    reward          | -26.121878 |\n",
      "-----------------------------------\n",
      "day: 8187, episode: 60\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 20195082.10\n",
      "total_reward: 19195082.10\n",
      "total_cost: 1006.16\n",
      "total_trades: 98464\n",
      "Sharpe: 0.598\n",
      "=================================\n",
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    episodes        | 8          |\n",
      "|    fps             | 56         |\n",
      "|    time_elapsed    | 1164       |\n",
      "|    total_timesteps | 65504      |\n",
      "| train/             |            |\n",
      "|    actor_loss      | 7.66e+05   |\n",
      "|    critic_loss     | 9.08e+05   |\n",
      "|    ent_coef        | 67.6       |\n",
      "|    ent_coef_loss   | -1.27e+03  |\n",
      "|    learning_rate   | 0.0001     |\n",
      "|    n_updates       | 65403      |\n",
      "|    reward          | -26.121878 |\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-01T15:35:13.726483Z",
     "start_time": "2024-12-01T15:35:13.697661Z"
    }
   },
   "cell_type": "code",
   "source": "trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac\") if if_using_sac else None",
   "id": "946eb240c5ea5d9f",
   "outputs": [],
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
