{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Stock NeurIPS2018 Part 2. Train\n",
    "\n",
    "This series is a reproduction of *the process in the paper Practical Deep Reinforcement Learning Approach for Stock Trading*. \n",
    "\n",
    "This is the second part of the NeurIPS2018 series, introducing how to use FinRL to make data into the gym form environment, and train DRL agents on it.\n",
    "\n",
    "Other demos can be found at the repo of [FinRL-Tutorials]((https://github.com/AI4Finance-Foundation/FinRL-Tutorials))."
   ],
   "id": "ca6a311812de640e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Part 1. Install Packages",
   "id": "7e740ba51db67d57"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T13:07:26.017105Z",
     "start_time": "2024-11-07T13:07:22.446226Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## install required packages\n",
    "!pip install swig\n",
    "!pip install wrds\n",
    "!pip install pyportfolioopt\n"
   ],
   "id": "da8c941660dcd212",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: swig in /Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRL-Tutorials-Learn/.venv/lib/python3.10/site-packages (4.2.1.post0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: wrds in /Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRL-Tutorials-Learn/.venv/lib/python3.10/site-packages (3.2.0)\r\n",
      "Requirement already satisfied: numpy<1.27,>=1.26 in /Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRL-Tutorials-Learn/.venv/lib/python3.10/site-packages (from wrds) (1.26.4)\r\n",
      "Requirement already satisfied: packaging<23.3 in /Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRL-Tutorials-Learn/.venv/lib/python3.10/site-packages (from wrds) (23.2)\r\n",
      "Requirement already satisfied: pandas<2.3,>=2.2 in /Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRL-Tutorials-Learn/.venv/lib/python3.10/site-packages (from wrds) (2.2.3)\r\n",
      "Requirement already satisfied: psycopg2-binary<2.10,>=2.9 in /Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRL-Tutorials-Learn/.venv/lib/python3.10/site-packages (from wrds) (2.9.10)\r\n",
      "Requirement already satisfied: scipy<1.13,>=1.12 in /Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRL-Tutorials-Learn/.venv/lib/python3.10/site-packages (from wrds) (1.12.0)\r\n",
      "Requirement already satisfied: sqlalchemy<2.1,>=2 in /Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRL-Tutorials-Learn/.venv/lib/python3.10/site-packages (from wrds) (2.0.36)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRL-Tutorials-Learn/.venv/lib/python3.10/site-packages (from pandas<2.3,>=2.2->wrds) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRL-Tutorials-Learn/.venv/lib/python3.10/site-packages (from pandas<2.3,>=2.2->wrds) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRL-Tutorials-Learn/.venv/lib/python3.10/site-packages (from pandas<2.3,>=2.2->wrds) (2024.2)\r\n",
      "Requirement already satisfied: typing-extensions>=4.6.0 in /Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRL-Tutorials-Learn/.venv/lib/python3.10/site-packages (from sqlalchemy<2.1,>=2->wrds) (4.12.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRL-Tutorials-Learn/.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<2.3,>=2.2->wrds) (1.16.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n",
      "Requirement already satisfied: pyportfolioopt in /Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRL-Tutorials-Learn/.venv/lib/python3.10/site-packages (1.5.5)\r\n",
      "Requirement already satisfied: cvxpy<2.0.0,>=1.1.19 in /Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRL-Tutorials-Learn/.venv/lib/python3.10/site-packages (from pyportfolioopt) (1.5.3)\r\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.22.4 in /Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRL-Tutorials-Learn/.venv/lib/python3.10/site-packages (from pyportfolioopt) (1.26.4)\r\n",
      "Requirement already satisfied: pandas>=0.19 in /Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRL-Tutorials-Learn/.venv/lib/python3.10/site-packages (from pyportfolioopt) (2.2.3)\r\n",
      "Requirement already satisfied: scipy<2.0,>=1.3 in /Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRL-Tutorials-Learn/.venv/lib/python3.10/site-packages (from pyportfolioopt) (1.12.0)\r\n",
      "Requirement already satisfied: osqp>=0.6.2 in /Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRL-Tutorials-Learn/.venv/lib/python3.10/site-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt) (0.6.7.post3)\r\n",
      "Requirement already satisfied: ecos>=2 in /Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRL-Tutorials-Learn/.venv/lib/python3.10/site-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt) (2.0.14)\r\n",
      "Requirement already satisfied: clarabel>=0.5.0 in /Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRL-Tutorials-Learn/.venv/lib/python3.10/site-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt) (0.9.0)\r\n",
      "Requirement already satisfied: scs>=3.2.4.post1 in /Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRL-Tutorials-Learn/.venv/lib/python3.10/site-packages (from cvxpy<2.0.0,>=1.1.19->pyportfolioopt) (3.2.7)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRL-Tutorials-Learn/.venv/lib/python3.10/site-packages (from pandas>=0.19->pyportfolioopt) (2.9.0.post0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRL-Tutorials-Learn/.venv/lib/python3.10/site-packages (from pandas>=0.19->pyportfolioopt) (2024.2)\r\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRL-Tutorials-Learn/.venv/lib/python3.10/site-packages (from pandas>=0.19->pyportfolioopt) (2024.2)\r\n",
      "Requirement already satisfied: qdldl in /Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRL-Tutorials-Learn/.venv/lib/python3.10/site-packages (from osqp>=0.6.2->cvxpy<2.0.0,>=1.1.19->pyportfolioopt) (0.1.7.post4)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/zhengshuang/Documents/ProgramingGuide/SourceCode/GithubRepo/everyfine/FinRLLearn/FinRL-Tutorials-Learn/.venv/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=0.19->pyportfolioopt) (1.16.0)\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip is available: \u001B[0m\u001B[31;49m23.2.1\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m24.3.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T13:10:28.943167Z",
     "start_time": "2024-11-07T13:10:26.507999Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from finrl.meta.env_stock_trading.env_stocktrading import StockTradingEnv\n",
    "from finrl.agents.stablebaselines3.models import DRLAgent\n",
    "from stable_baselines3.common.logger import  configure\n",
    "from finrl import config_tickers\n",
    "from finrl.main import check_and_make_directories\n",
    "from finrl.config import INDICATORS, TRAINED_MODEL_DIR, RESULTS_DIR\n",
    "\n",
    "check_and_make_directories([TRAINED_MODEL_DIR])"
   ],
   "id": "168ed1c9145ffba2",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Part 2. Build A Market Environment in OpenAI Gym-style",
   "id": "fe4be4bf7815a4cd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "![rl_diagram_transparent_bg.png](data:image/png;base64,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)",
   "id": "fb12cf235e323143"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The core element in reinforcement learning are **agent** and **environment**. You can understand RL as the following process: \n",
    "\n",
    "The agent is active in a world, which is the environment. It observe its current condition as a **state**, and is allowed to do certain **actions**. After the agent execute an action, it will arrive at a new state. At the same time, the environment will have feedback to the agent called **reward**, a numerical signal that tells how good or bad the new state is. As the figure above, agent and environment will keep doing this interaction.\n",
    "\n",
    "The goal of agent is to get as much cumulative reward as possible. Reinforcement learning is the method that agent learns to improve its behavior and achieve that goal."
   ],
   "id": "e5b9059d67c12ab6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "To achieve this in Python, we follow the OpenAI gym style to build the stock data into environment.\n",
    "\n",
    "state-action-reward are specified as follows:\n",
    "\n",
    "* **State s**: The state space represents an agent's perception of the market environment. Just like a human trader analyzing various information, here our agent passively observes the price data and technical indicators based on the past data. It will learn by interacting with the market environment (usually by replaying historical data).\n",
    "\n",
    "* **Action a**: The action space includes allowed actions that an agent can take at each state. For example, a ∈ {−1, 0, 1}, where −1, 0, 1 represent\n",
    "selling, holding, and buying. When an action operates multiple shares, a ∈{−k, ..., −1, 0, 1, ..., k}, e.g.. \"Buy 10 shares of AAPL\" or \"Sell 10 shares of AAPL\" are 10 or −10, respectively\n",
    "\n",
    "* **Reward function r(s, a, s′)**: Reward is an incentive for an agent to learn a better policy. For example, it can be the change of the portfolio value when taking a at state s and arriving at new state s',  i.e., r(s, a, s′) = v′ − v, where v′ and v represent the portfolio values at state s′ and s, respectively\n",
    "\n",
    "\n",
    "**Market environment**: 30 constituent stocks of Dow Jones Industrial Average (DJIA) index. Accessed at the starting date of the testing period."
   ],
   "id": "1610eee85cc92ba1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Read data\n",
    "\n",
    "We first read the .csv file of our training data into dataframe."
   ],
   "id": "837e9a8f1d36ea9b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T13:22:21.375778Z",
     "start_time": "2024-11-07T13:22:21.214922Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train = pd.read_csv('train_data.csv')\n",
    "\n",
    "# If you are not using the data generated from part 1 of this tutorial, make sure \n",
    "# it has the columns and index in the form that could be make into the environment. \n",
    "# Then you can comment and skip the following two lines.\n",
    "train = train.set_index(train.columns[0])\n",
    "train.index.names = ['']"
   ],
   "id": "916d29e812a21d3e",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-07T13:24:31.871125Z",
     "start_time": "2024-11-07T13:24:31.864687Z"
    }
   },
   "cell_type": "code",
   "source": "train.tic.unique()",
   "id": "9765d37d2c247458",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AAPL', 'AMGN', 'AXP', 'BA', 'CAT', 'CRM', 'CSCO', 'CVX', 'DIS',\n",
       "       'GS', 'HD', 'HON', 'IBM', 'INTC', 'JNJ', 'JPM', 'KO', 'MCD', 'MMM',\n",
       "       'MRK', 'MSFT', 'NKE', 'PG', 'TRV', 'UNH', 'V', 'VZ', 'WBA', 'WMT'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Construct the environment\n",
    "\n",
    "Calculate and specify the parameters we need for constructing the environment."
   ],
   "id": "8bc3915936148526"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T13:39:17.441856Z",
     "start_time": "2024-11-08T13:39:17.436975Z"
    }
   },
   "cell_type": "code",
   "source": [
    "stock_dimension = len(train.tic.unique())\n",
    "state_space = 1 + 2*stock_dimension + len(INDICATORS)*stock_dimension\n",
    "print(f\"Stock Dimension: {stock_dimension}, State Space: {state_space}\")"
   ],
   "id": "b55f08b2deed794d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stock Dimension: 29, State Space: 291\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T13:40:05.442477Z",
     "start_time": "2024-11-08T13:40:05.439612Z"
    }
   },
   "cell_type": "code",
   "source": "stock_dimension",
   "id": "29387b85097bb939",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T13:39:09.579744Z",
     "start_time": "2024-11-08T13:39:09.573613Z"
    }
   },
   "cell_type": "code",
   "source": "INDICATORS",
   "id": "70edae440eeab6df",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['macd',\n",
       " 'boll_ub',\n",
       " 'boll_lb',\n",
       " 'rsi_30',\n",
       " 'cci_30',\n",
       " 'dx_30',\n",
       " 'close_30_sma',\n",
       " 'close_60_sma']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T13:41:05.258481Z",
     "start_time": "2024-11-08T13:41:05.254889Z"
    }
   },
   "cell_type": "code",
   "source": [
    "buy_cost_list = sell_cost_list = [0.001] * stock_dimension\n",
    "buy_cost_list"
   ],
   "id": "f50d069997a544e6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001,\n",
       " 0.001]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T13:45:37.448467Z",
     "start_time": "2024-11-08T13:45:37.441096Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_stock_shares = [0] * stock_dimension\n",
    "env_kwargs = {\n",
    "    \"hmax\": 100,\n",
    "    \"initial_amount\": 1000000,\n",
    "    \"num_stock_shares\": num_stock_shares,\n",
    "    \"buy_cost_pct\": buy_cost_list,\n",
    "    \"sell_cost_pct\": sell_cost_list,\n",
    "    \"state_space\": state_space,\n",
    "    \"stock_dim\": stock_dimension,\n",
    "    \"tech_indicator_list\": INDICATORS,\n",
    "    \"action_space\": stock_dimension,\n",
    "    \"reward_scaling\": 1e-4,\n",
    "}\n",
    "\n",
    "e_train_gym = StockTradingEnv(df = train, **env_kwargs)"
   ],
   "id": "b20422de148dc48c",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Environment for training",
   "id": "3d1e21832d092980"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T13:46:27.278308Z",
     "start_time": "2024-11-08T13:46:27.270984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "env_train, _ = e_train_gym.get_sb_env()\n",
    "print(type(env_train))"
   ],
   "id": "140c61706cfb645b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv'>\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Part 3: Train DRL Agents\n",
    "* Here, the DRL algorithms are from **[Stable Baselines 3](https://stable-baselines3.readthedocs.io/en/master/)**. It's a library that implemented popular DRL algorithms using pytorch, succeeding to its old version: Stable Baselines.\n",
    "* Users are also encouraged to try **[ElegantRL](https://github.com/AI4Finance-Foundation/ElegantRL)** and **[Ray RLlib](https://github.com/ray-project/ray)**."
   ],
   "id": "f90990a2982c08c8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T13:49:30.056421Z",
     "start_time": "2024-11-08T13:49:30.053998Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "\n",
    "# set the corresponding values to 'True' for the algorithms that you want to use\n",
    "if_using_a2c = True\n",
    "if_using_ddpg = True\n",
    "if_using_ppo = True\n",
    "if_using_td3 = True\n",
    "if_using_sac = True"
   ],
   "id": "eef8bf0fb445215e",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Agent Training: 5 algorithms (A2C, DDPG, PPO, TD3, SAC)",
   "id": "89bf675aa9bb98ff"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Agent 1: A2C",
   "id": "26bbffbe273973d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T13:52:33.630202Z",
     "start_time": "2024-11-08T13:52:32.780035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_a2c = agent.get_model('a2c')\n",
    "\n",
    "if if_using_a2c:\n",
    "    # set up logger\n",
    "    tmp_path = RESULTS_DIR + '/a2c'\n",
    "    new_logger_a2c = configure(tmp_path, [\"stdout\",\"csv\",\"tensorboard\"])\n",
    "    # set new logger\n",
    "    model_a2c.set_logger(new_logger_a2c)"
   ],
   "id": "f6d37d9f5cc3c8c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 5, 'ent_coef': 0.01, 'learning_rate': 0.0007}\n",
      "Using cpu device\n",
      "Logging to results/a2c\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T13:59:40.812124Z",
     "start_time": "2024-11-08T13:55:06.782564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trained_a2c = agent.train_model(model=model_a2c,\n",
    "                                tb_log_name='a2c',\n",
    "                                total_timesteps=50000) if if_using_a2c else None"
   ],
   "id": "4c3bf6eed20538b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 179      |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 2        |\n",
      "|    total_timesteps    | 500      |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.3    |\n",
      "|    explained_variance | -0.406   |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -68.3    |\n",
      "|    reward             | 1.52373  |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 4.16     |\n",
      "------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 178          |\n",
      "|    iterations         | 200          |\n",
      "|    time_elapsed       | 5            |\n",
      "|    total_timesteps    | 1000         |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -41.3        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 199          |\n",
      "|    policy_loss        | -19.5        |\n",
      "|    reward             | -0.059543546 |\n",
      "|    std                | 1.01         |\n",
      "|    value_loss         | 0.452        |\n",
      "----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 179         |\n",
      "|    iterations         | 300         |\n",
      "|    time_elapsed       | 8           |\n",
      "|    total_timesteps    | 1500        |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.3       |\n",
      "|    explained_variance | -0.144      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 299         |\n",
      "|    policy_loss        | -75         |\n",
      "|    reward             | -0.27121237 |\n",
      "|    std                | 1.01        |\n",
      "|    value_loss         | 6.36        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 180       |\n",
      "|    iterations         | 400       |\n",
      "|    time_elapsed       | 11        |\n",
      "|    total_timesteps    | 2000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0.11      |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 399       |\n",
      "|    policy_loss        | -10.3     |\n",
      "|    reward             | 1.1242657 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 1.66      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 181       |\n",
      "|    iterations         | 500       |\n",
      "|    time_elapsed       | 13        |\n",
      "|    total_timesteps    | 2500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 499       |\n",
      "|    policy_loss        | 331       |\n",
      "|    reward             | 4.2218494 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 69.2      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 181        |\n",
      "|    iterations         | 600        |\n",
      "|    time_elapsed       | 16         |\n",
      "|    total_timesteps    | 3000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0.161      |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 599        |\n",
      "|    policy_loss        | 141        |\n",
      "|    reward             | -2.0442772 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 14.4       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 181       |\n",
      "|    iterations         | 700       |\n",
      "|    time_elapsed       | 19        |\n",
      "|    total_timesteps    | 3500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0.0421    |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 699       |\n",
      "|    policy_loss        | -32.2     |\n",
      "|    reward             | -0.776623 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 2.14      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 181        |\n",
      "|    iterations         | 800        |\n",
      "|    time_elapsed       | 22         |\n",
      "|    total_timesteps    | 4000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 799        |\n",
      "|    policy_loss        | 35.5       |\n",
      "|    reward             | -1.6514686 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3.04       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 181        |\n",
      "|    iterations         | 900        |\n",
      "|    time_elapsed       | 24         |\n",
      "|    total_timesteps    | 4500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 899        |\n",
      "|    policy_loss        | 92.2       |\n",
      "|    reward             | -1.4725136 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 6.46       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 181        |\n",
      "|    iterations         | 1000       |\n",
      "|    time_elapsed       | 27         |\n",
      "|    total_timesteps    | 5000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 999        |\n",
      "|    policy_loss        | -44.3      |\n",
      "|    reward             | 0.88321126 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.75       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 181       |\n",
      "|    iterations         | 1100      |\n",
      "|    time_elapsed       | 30        |\n",
      "|    total_timesteps    | 5500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.2     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1099      |\n",
      "|    policy_loss        | 79.7      |\n",
      "|    reward             | 0.5587276 |\n",
      "|    std                | 1         |\n",
      "|    value_loss         | 6         |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 181        |\n",
      "|    iterations         | 1200       |\n",
      "|    time_elapsed       | 32         |\n",
      "|    total_timesteps    | 6000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1199       |\n",
      "|    policy_loss        | -57.4      |\n",
      "|    reward             | 0.95431703 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 3.68       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 182        |\n",
      "|    iterations         | 1300       |\n",
      "|    time_elapsed       | 35         |\n",
      "|    total_timesteps    | 6500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1299       |\n",
      "|    policy_loss        | 136        |\n",
      "|    reward             | -4.0318246 |\n",
      "|    std                | 1          |\n",
      "|    value_loss         | 14.4       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 182        |\n",
      "|    iterations         | 1400       |\n",
      "|    time_elapsed       | 38         |\n",
      "|    total_timesteps    | 7000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.3      |\n",
      "|    explained_variance | -0.0287    |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1399       |\n",
      "|    policy_loss        | 36.9       |\n",
      "|    reward             | 0.90419555 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 1.46       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 182      |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 41       |\n",
      "|    total_timesteps    | 7500     |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.3    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 202      |\n",
      "|    reward             | 5.311474 |\n",
      "|    std                | 1.01     |\n",
      "|    value_loss         | 25.1     |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 182       |\n",
      "|    iterations         | 1600      |\n",
      "|    time_elapsed       | 43        |\n",
      "|    total_timesteps    | 8000      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1599      |\n",
      "|    policy_loss        | 93.5      |\n",
      "|    reward             | 3.5731444 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 6.61      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 182       |\n",
      "|    iterations         | 1700      |\n",
      "|    time_elapsed       | 46        |\n",
      "|    total_timesteps    | 8500      |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.3     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 1699      |\n",
      "|    policy_loss        | 65.3      |\n",
      "|    reward             | 2.7052212 |\n",
      "|    std                | 1.01      |\n",
      "|    value_loss         | 5.58      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 182        |\n",
      "|    iterations         | 1800       |\n",
      "|    time_elapsed       | 49         |\n",
      "|    total_timesteps    | 9000       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1799       |\n",
      "|    policy_loss        | 2.42       |\n",
      "|    reward             | 0.95354146 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.177      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 182        |\n",
      "|    iterations         | 1900       |\n",
      "|    time_elapsed       | 52         |\n",
      "|    total_timesteps    | 9500       |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1899       |\n",
      "|    policy_loss        | -4.11      |\n",
      "|    reward             | -0.8382266 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 0.87       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 182        |\n",
      "|    iterations         | 2000       |\n",
      "|    time_elapsed       | 54         |\n",
      "|    total_timesteps    | 10000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.5      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 1999       |\n",
      "|    policy_loss        | 31.5       |\n",
      "|    reward             | 0.09371303 |\n",
      "|    std                | 1.01       |\n",
      "|    value_loss         | 2.32       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 182      |\n",
      "|    iterations         | 2100     |\n",
      "|    time_elapsed       | 57       |\n",
      "|    total_timesteps    | 10500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -41.6    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2099     |\n",
      "|    policy_loss        | -22      |\n",
      "|    reward             | 2.370801 |\n",
      "|    std                | 1.02     |\n",
      "|    value_loss         | 0.354    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 182       |\n",
      "|    iterations         | 2200      |\n",
      "|    time_elapsed       | 60        |\n",
      "|    total_timesteps    | 11000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2199      |\n",
      "|    policy_loss        | -10.4     |\n",
      "|    reward             | -3.801432 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 2.79      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 181       |\n",
      "|    iterations         | 2300      |\n",
      "|    time_elapsed       | 63        |\n",
      "|    total_timesteps    | 11500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 4.66e-05  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2299      |\n",
      "|    policy_loss        | -1.18e+03 |\n",
      "|    reward             | 6.291082  |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 763       |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 181         |\n",
      "|    iterations         | 2400        |\n",
      "|    time_elapsed       | 65          |\n",
      "|    total_timesteps    | 12000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.6       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2399        |\n",
      "|    policy_loss        | 74.6        |\n",
      "|    reward             | -0.36560506 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 5.79        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 181       |\n",
      "|    iterations         | 2500      |\n",
      "|    time_elapsed       | 68        |\n",
      "|    total_timesteps    | 12500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.6     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2499      |\n",
      "|    policy_loss        | -9.95     |\n",
      "|    reward             | 2.1897416 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.28      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 182         |\n",
      "|    iterations         | 2600        |\n",
      "|    time_elapsed       | 71          |\n",
      "|    total_timesteps    | 13000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -41.6       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 2599        |\n",
      "|    policy_loss        | -30.8       |\n",
      "|    reward             | -0.17086758 |\n",
      "|    std                | 1.02        |\n",
      "|    value_loss         | 0.756       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 182       |\n",
      "|    iterations         | 2700      |\n",
      "|    time_elapsed       | 74        |\n",
      "|    total_timesteps    | 13500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.7     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 2699      |\n",
      "|    policy_loss        | -30.9     |\n",
      "|    reward             | 0.9164081 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 1.08      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 182        |\n",
      "|    iterations         | 2800       |\n",
      "|    time_elapsed       | 76         |\n",
      "|    total_timesteps    | 14000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2799       |\n",
      "|    policy_loss        | 178        |\n",
      "|    reward             | -0.8268668 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 23.1       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 182        |\n",
      "|    iterations         | 2900       |\n",
      "|    time_elapsed       | 79         |\n",
      "|    total_timesteps    | 14500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.7      |\n",
      "|    explained_variance | -0.273     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2899       |\n",
      "|    policy_loss        | -177       |\n",
      "|    reward             | -1.8781356 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 19.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 182        |\n",
      "|    iterations         | 3000       |\n",
      "|    time_elapsed       | 82         |\n",
      "|    total_timesteps    | 15000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 2999       |\n",
      "|    policy_loss        | 62.8       |\n",
      "|    reward             | 0.42055294 |\n",
      "|    std                | 1.02       |\n",
      "|    value_loss         | 2.42       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 182       |\n",
      "|    iterations         | 3100      |\n",
      "|    time_elapsed       | 85        |\n",
      "|    total_timesteps    | 15500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -41.8     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3099      |\n",
      "|    policy_loss        | 10.8      |\n",
      "|    reward             | 0.2195209 |\n",
      "|    std                | 1.02      |\n",
      "|    value_loss         | 0.366     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 182        |\n",
      "|    iterations         | 3200       |\n",
      "|    time_elapsed       | 87         |\n",
      "|    total_timesteps    | 16000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3199       |\n",
      "|    policy_loss        | -160       |\n",
      "|    reward             | -1.3848015 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 14.8       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 182        |\n",
      "|    iterations         | 3300       |\n",
      "|    time_elapsed       | 90         |\n",
      "|    total_timesteps    | 16500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -41.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3299       |\n",
      "|    policy_loss        | -45.7      |\n",
      "|    reward             | 0.57994086 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.69       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 182         |\n",
      "|    iterations         | 3400        |\n",
      "|    time_elapsed       | 93          |\n",
      "|    total_timesteps    | 17000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3399        |\n",
      "|    policy_loss        | 231         |\n",
      "|    reward             | 0.036164332 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 31.3        |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 182         |\n",
      "|    iterations         | 3500        |\n",
      "|    time_elapsed       | 96          |\n",
      "|    total_timesteps    | 17500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42         |\n",
      "|    explained_variance | 0.0414      |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 3499        |\n",
      "|    policy_loss        | 115         |\n",
      "|    reward             | -0.65572816 |\n",
      "|    std                | 1.03        |\n",
      "|    value_loss         | 9.16        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 182        |\n",
      "|    iterations         | 3600       |\n",
      "|    time_elapsed       | 98         |\n",
      "|    total_timesteps    | 18000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42        |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3599       |\n",
      "|    policy_loss        | -32.6      |\n",
      "|    reward             | 0.72082084 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 1.48       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 182        |\n",
      "|    iterations         | 3700       |\n",
      "|    time_elapsed       | 101        |\n",
      "|    total_timesteps    | 18500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3699       |\n",
      "|    policy_loss        | 152        |\n",
      "|    reward             | 0.10315714 |\n",
      "|    std                | 1.03       |\n",
      "|    value_loss         | 15.6       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 182        |\n",
      "|    iterations         | 3800       |\n",
      "|    time_elapsed       | 104        |\n",
      "|    total_timesteps    | 19000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.1      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3799       |\n",
      "|    policy_loss        | 24.9       |\n",
      "|    reward             | 0.97068924 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 0.947      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 182        |\n",
      "|    iterations         | 3900       |\n",
      "|    time_elapsed       | 107        |\n",
      "|    total_timesteps    | 19500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.2      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 3899       |\n",
      "|    policy_loss        | 84.8       |\n",
      "|    reward             | 0.06472053 |\n",
      "|    std                | 1.04       |\n",
      "|    value_loss         | 4.51       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 182      |\n",
      "|    iterations         | 4000     |\n",
      "|    time_elapsed       | 109      |\n",
      "|    total_timesteps    | 20000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3999     |\n",
      "|    policy_loss        | 29.3     |\n",
      "|    reward             | 2.364968 |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 0.795    |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 182         |\n",
      "|    iterations         | 4100        |\n",
      "|    time_elapsed       | 112         |\n",
      "|    total_timesteps    | 20500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.3       |\n",
      "|    explained_variance | 5.96e-08    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 4099        |\n",
      "|    policy_loss        | -37.5       |\n",
      "|    reward             | 0.086727574 |\n",
      "|    std                | 1.04        |\n",
      "|    value_loss         | 1.68        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 182       |\n",
      "|    iterations         | 4200      |\n",
      "|    time_elapsed       | 115       |\n",
      "|    total_timesteps    | 21000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4199      |\n",
      "|    policy_loss        | -123      |\n",
      "|    reward             | 1.1316164 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 9.82      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 182        |\n",
      "|    iterations         | 4300       |\n",
      "|    time_elapsed       | 117        |\n",
      "|    total_timesteps    | 21500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4299       |\n",
      "|    policy_loss        | -1.58      |\n",
      "|    reward             | -0.5220481 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 0.161      |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 182        |\n",
      "|    iterations         | 4400       |\n",
      "|    time_elapsed       | 120        |\n",
      "|    total_timesteps    | 22000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.5      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4399       |\n",
      "|    policy_loss        | 56.1       |\n",
      "|    reward             | -1.3194045 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 4.33       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 182        |\n",
      "|    iterations         | 4500       |\n",
      "|    time_elapsed       | 123        |\n",
      "|    total_timesteps    | 22500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4499       |\n",
      "|    policy_loss        | 138        |\n",
      "|    reward             | -1.5819061 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 10.7       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 182      |\n",
      "|    iterations         | 4600     |\n",
      "|    time_elapsed       | 126      |\n",
      "|    total_timesteps    | 23000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.4    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4599     |\n",
      "|    policy_loss        | 12.6     |\n",
      "|    reward             | 1.612227 |\n",
      "|    std                | 1.05     |\n",
      "|    value_loss         | 0.592    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 182       |\n",
      "|    iterations         | 4700      |\n",
      "|    time_elapsed       | 128       |\n",
      "|    total_timesteps    | 23500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4699      |\n",
      "|    policy_loss        | -49.1     |\n",
      "|    reward             | 3.0498953 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 8.41      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 182       |\n",
      "|    iterations         | 4800      |\n",
      "|    time_elapsed       | 131       |\n",
      "|    total_timesteps    | 24000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | -0.0995   |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 4799      |\n",
      "|    policy_loss        | -42.3     |\n",
      "|    reward             | 0.5403945 |\n",
      "|    std                | 1.04      |\n",
      "|    value_loss         | 1.17      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 182      |\n",
      "|    iterations         | 4900     |\n",
      "|    time_elapsed       | 134      |\n",
      "|    total_timesteps    | 24500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.4    |\n",
      "|    explained_variance | 1.19e-07 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4899     |\n",
      "|    policy_loss        | -92.5    |\n",
      "|    reward             | 1.19193  |\n",
      "|    std                | 1.04     |\n",
      "|    value_loss         | 5.74     |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 182        |\n",
      "|    iterations         | 5000       |\n",
      "|    time_elapsed       | 137        |\n",
      "|    total_timesteps    | 25000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.4      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 4999       |\n",
      "|    policy_loss        | 84.2       |\n",
      "|    reward             | -4.7306786 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 5.36       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 182       |\n",
      "|    iterations         | 5100      |\n",
      "|    time_elapsed       | 139       |\n",
      "|    total_timesteps    | 25500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5099      |\n",
      "|    policy_loss        | 10.3      |\n",
      "|    reward             | 3.6660435 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 23.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 182       |\n",
      "|    iterations         | 5200      |\n",
      "|    time_elapsed       | 142       |\n",
      "|    total_timesteps    | 26000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.4     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5199      |\n",
      "|    policy_loss        | -383      |\n",
      "|    reward             | -0.336024 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 103       |\n",
      "-------------------------------------\n",
      "day: 2892, episode: 10\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3920775.25\n",
      "total_reward: 2920775.25\n",
      "total_cost: 3092.52\n",
      "total_trades: 46219\n",
      "Sharpe: 0.855\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                 |               |\n",
      "|    fps                | 182           |\n",
      "|    iterations         | 5300          |\n",
      "|    time_elapsed       | 145           |\n",
      "|    total_timesteps    | 26500         |\n",
      "| train/                |               |\n",
      "|    entropy_loss       | -42.4         |\n",
      "|    explained_variance | 0             |\n",
      "|    learning_rate      | 0.0007        |\n",
      "|    n_updates          | 5299          |\n",
      "|    policy_loss        | -17.3         |\n",
      "|    reward             | -0.0017296011 |\n",
      "|    std                | 1.05          |\n",
      "|    value_loss         | 0.547         |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 182         |\n",
      "|    iterations         | 5400        |\n",
      "|    time_elapsed       | 148         |\n",
      "|    total_timesteps    | 27000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.4       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5399        |\n",
      "|    policy_loss        | 24          |\n",
      "|    reward             | -0.63261884 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 2.54        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 182       |\n",
      "|    iterations         | 5500      |\n",
      "|    time_elapsed       | 150       |\n",
      "|    total_timesteps    | 27500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.5     |\n",
      "|    explained_variance | 1.19e-07  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5499      |\n",
      "|    policy_loss        | -189      |\n",
      "|    reward             | 0.1818555 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 41.8      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 182         |\n",
      "|    iterations         | 5600        |\n",
      "|    time_elapsed       | 153         |\n",
      "|    total_timesteps    | 28000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.5       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5599        |\n",
      "|    policy_loss        | -182        |\n",
      "|    reward             | -0.76656896 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 19.6        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 182       |\n",
      "|    iterations         | 5700      |\n",
      "|    time_elapsed       | 156       |\n",
      "|    total_timesteps    | 28500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5699      |\n",
      "|    policy_loss        | -149      |\n",
      "|    reward             | 0.7884841 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 16.5      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 182       |\n",
      "|    iterations         | 5800      |\n",
      "|    time_elapsed       | 159       |\n",
      "|    total_timesteps    | 29000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.6     |\n",
      "|    explained_variance | -0.44     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 5799      |\n",
      "|    policy_loss        | 28.5      |\n",
      "|    reward             | -2.397213 |\n",
      "|    std                | 1.05      |\n",
      "|    value_loss         | 5.55      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 182         |\n",
      "|    iterations         | 5900        |\n",
      "|    time_elapsed       | 161         |\n",
      "|    total_timesteps    | 29500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 5899        |\n",
      "|    policy_loss        | 23.9        |\n",
      "|    reward             | -0.19065112 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 0.531       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 182        |\n",
      "|    iterations         | 6000       |\n",
      "|    time_elapsed       | 164        |\n",
      "|    total_timesteps    | 30000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0.0101     |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 5999       |\n",
      "|    policy_loss        | 101        |\n",
      "|    reward             | -0.6748308 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 8.26       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 182        |\n",
      "|    iterations         | 6100       |\n",
      "|    time_elapsed       | 167        |\n",
      "|    total_timesteps    | 30500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6099       |\n",
      "|    policy_loss        | 88.4       |\n",
      "|    reward             | 0.22583283 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 8.78       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 182         |\n",
      "|    iterations         | 6200        |\n",
      "|    time_elapsed       | 170         |\n",
      "|    total_timesteps    | 31000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.7       |\n",
      "|    explained_variance | 1.19e-07    |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6199        |\n",
      "|    policy_loss        | -199        |\n",
      "|    reward             | -0.18962575 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 22.5        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 182        |\n",
      "|    iterations         | 6300       |\n",
      "|    time_elapsed       | 172        |\n",
      "|    total_timesteps    | 31500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6299       |\n",
      "|    policy_loss        | 135        |\n",
      "|    reward             | -1.3985502 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 14.9       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 182         |\n",
      "|    iterations         | 6400        |\n",
      "|    time_elapsed       | 175         |\n",
      "|    total_timesteps    | 32000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6399        |\n",
      "|    policy_loss        | 54.6        |\n",
      "|    reward             | -0.15544263 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 2.87        |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 182        |\n",
      "|    iterations         | 6500       |\n",
      "|    time_elapsed       | 178        |\n",
      "|    total_timesteps    | 32500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6499       |\n",
      "|    policy_loss        | 53.9       |\n",
      "|    reward             | -2.4424527 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 4.47       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 182        |\n",
      "|    iterations         | 6600       |\n",
      "|    time_elapsed       | 180        |\n",
      "|    total_timesteps    | 33000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6599       |\n",
      "|    policy_loss        | -104       |\n",
      "|    reward             | 0.55493873 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 7.28       |\n",
      "--------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 182      |\n",
      "|    iterations         | 6700     |\n",
      "|    time_elapsed       | 183      |\n",
      "|    total_timesteps    | 33500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -42.6    |\n",
      "|    explained_variance | -0.00904 |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6699     |\n",
      "|    policy_loss        | -429     |\n",
      "|    reward             | -5.81576 |\n",
      "|    std                | 1.05     |\n",
      "|    value_loss         | 193      |\n",
      "------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 182        |\n",
      "|    iterations         | 6800       |\n",
      "|    time_elapsed       | 186        |\n",
      "|    total_timesteps    | 34000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 1.19e-07   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6799       |\n",
      "|    policy_loss        | -165       |\n",
      "|    reward             | -0.3654488 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 15.6       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 182        |\n",
      "|    iterations         | 6900       |\n",
      "|    time_elapsed       | 189        |\n",
      "|    total_timesteps    | 34500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.6      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 6899       |\n",
      "|    policy_loss        | -42        |\n",
      "|    reward             | 0.16337235 |\n",
      "|    std                | 1.05       |\n",
      "|    value_loss         | 6.07       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 182         |\n",
      "|    iterations         | 7000        |\n",
      "|    time_elapsed       | 191         |\n",
      "|    total_timesteps    | 35000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.6       |\n",
      "|    explained_variance | -1.19e-07   |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 6999        |\n",
      "|    policy_loss        | -42.7       |\n",
      "|    reward             | 0.057427503 |\n",
      "|    std                | 1.05        |\n",
      "|    value_loss         | 2.44        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 182       |\n",
      "|    iterations         | 7100      |\n",
      "|    time_elapsed       | 194       |\n",
      "|    total_timesteps    | 35500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7099      |\n",
      "|    policy_loss        | 33.4      |\n",
      "|    reward             | 1.3531263 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 1.29      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 182       |\n",
      "|    iterations         | 7200      |\n",
      "|    time_elapsed       | 197       |\n",
      "|    total_timesteps    | 36000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.7     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7199      |\n",
      "|    policy_loss        | 20.2      |\n",
      "|    reward             | 1.7138605 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 1.05      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 182        |\n",
      "|    iterations         | 7300       |\n",
      "|    time_elapsed       | 200        |\n",
      "|    total_timesteps    | 36500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.7      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7299       |\n",
      "|    policy_loss        | -76.9      |\n",
      "|    reward             | -2.3317428 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 6.79       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 182        |\n",
      "|    iterations         | 7400       |\n",
      "|    time_elapsed       | 202        |\n",
      "|    total_timesteps    | 37000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7399       |\n",
      "|    policy_loss        | 130        |\n",
      "|    reward             | -1.3946104 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 10.9       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 182        |\n",
      "|    iterations         | 7500       |\n",
      "|    time_elapsed       | 205        |\n",
      "|    total_timesteps    | 37500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.8      |\n",
      "|    explained_variance | -0.00283   |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7499       |\n",
      "|    policy_loss        | 197        |\n",
      "|    reward             | -14.179357 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 28.2       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 182         |\n",
      "|    iterations         | 7600        |\n",
      "|    time_elapsed       | 208         |\n",
      "|    total_timesteps    | 38000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.8       |\n",
      "|    explained_variance | 0.015       |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 7599        |\n",
      "|    policy_loss        | 134         |\n",
      "|    reward             | -0.06304253 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 10.1        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 182       |\n",
      "|    iterations         | 7700      |\n",
      "|    time_elapsed       | 211       |\n",
      "|    total_timesteps    | 38500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7699      |\n",
      "|    policy_loss        | 6.43      |\n",
      "|    reward             | 1.1380314 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 0.221     |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 182        |\n",
      "|    iterations         | 7800       |\n",
      "|    time_elapsed       | 213        |\n",
      "|    total_timesteps    | 39000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 7799       |\n",
      "|    policy_loss        | -54.7      |\n",
      "|    reward             | -0.8557576 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 2.39       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 182       |\n",
      "|    iterations         | 7900      |\n",
      "|    time_elapsed       | 216       |\n",
      "|    total_timesteps    | 39500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7899      |\n",
      "|    policy_loss        | 120       |\n",
      "|    reward             | 2.1670933 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 22.3      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 182       |\n",
      "|    iterations         | 8000      |\n",
      "|    time_elapsed       | 219       |\n",
      "|    total_timesteps    | 40000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 7999      |\n",
      "|    policy_loss        | -206      |\n",
      "|    reward             | 4.4221563 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 22.4      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 182       |\n",
      "|    iterations         | 8100      |\n",
      "|    time_elapsed       | 221       |\n",
      "|    total_timesteps    | 40500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8099      |\n",
      "|    policy_loss        | 20.4      |\n",
      "|    reward             | -4.747896 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 39.1      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 182         |\n",
      "|    iterations         | 8200        |\n",
      "|    time_elapsed       | 224         |\n",
      "|    total_timesteps    | 41000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8199        |\n",
      "|    policy_loss        | -43.5       |\n",
      "|    reward             | -0.33810556 |\n",
      "|    std                | 1.07        |\n",
      "|    value_loss         | 1.17        |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 182       |\n",
      "|    iterations         | 8300      |\n",
      "|    time_elapsed       | 227       |\n",
      "|    total_timesteps    | 41500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8299      |\n",
      "|    policy_loss        | 82.3      |\n",
      "|    reward             | 1.6498492 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 5.01      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 182       |\n",
      "|    iterations         | 8400      |\n",
      "|    time_elapsed       | 230       |\n",
      "|    total_timesteps    | 42000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8399      |\n",
      "|    policy_loss        | 46.9      |\n",
      "|    reward             | 1.6985408 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 1.41      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 182       |\n",
      "|    iterations         | 8500      |\n",
      "|    time_elapsed       | 232       |\n",
      "|    total_timesteps    | 42500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.9     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8499      |\n",
      "|    policy_loss        | 43.7      |\n",
      "|    reward             | 1.8882315 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 2.05      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 182        |\n",
      "|    iterations         | 8600       |\n",
      "|    time_elapsed       | 235        |\n",
      "|    total_timesteps    | 43000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | -1.19e-07  |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8599       |\n",
      "|    policy_loss        | -70.8      |\n",
      "|    reward             | -3.0623486 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 17.2       |\n",
      "--------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 182       |\n",
      "|    iterations         | 8700      |\n",
      "|    time_elapsed       | 238       |\n",
      "|    total_timesteps    | 43500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -42.8     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 8699      |\n",
      "|    policy_loss        | 49.6      |\n",
      "|    reward             | 1.6349751 |\n",
      "|    std                | 1.06      |\n",
      "|    value_loss         | 3.83      |\n",
      "-------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 182         |\n",
      "|    iterations         | 8800        |\n",
      "|    time_elapsed       | 241         |\n",
      "|    total_timesteps    | 44000       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -42.9       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 8799        |\n",
      "|    policy_loss        | -11         |\n",
      "|    reward             | -0.09496065 |\n",
      "|    std                | 1.06        |\n",
      "|    value_loss         | 0.321       |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 182        |\n",
      "|    iterations         | 8900       |\n",
      "|    time_elapsed       | 243        |\n",
      "|    total_timesteps    | 44500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8899       |\n",
      "|    policy_loss        | 32.9       |\n",
      "|    reward             | 0.09250422 |\n",
      "|    std                | 1.06       |\n",
      "|    value_loss         | 1.72       |\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 182        |\n",
      "|    iterations         | 9000       |\n",
      "|    time_elapsed       | 246        |\n",
      "|    total_timesteps    | 45000      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -42.9      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 8999       |\n",
      "|    policy_loss        | 25.8       |\n",
      "|    reward             | 0.57738954 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 2.34       |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 182         |\n",
      "|    iterations         | 9100        |\n",
      "|    time_elapsed       | 249         |\n",
      "|    total_timesteps    | 45500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -43         |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9099        |\n",
      "|    policy_loss        | 23.9        |\n",
      "|    reward             | -0.77547675 |\n",
      "|    std                | 1.07        |\n",
      "|    value_loss         | 0.949       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 182       |\n",
      "|    iterations         | 9200      |\n",
      "|    time_elapsed       | 252       |\n",
      "|    total_timesteps    | 46000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43       |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9199      |\n",
      "|    policy_loss        | -62.7     |\n",
      "|    reward             | 1.1683395 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 7.43      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 182       |\n",
      "|    iterations         | 9300      |\n",
      "|    time_elapsed       | 254       |\n",
      "|    total_timesteps    | 46500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.1     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9299      |\n",
      "|    policy_loss        | -142      |\n",
      "|    reward             | 1.6490413 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 12.7      |\n",
      "-------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 182       |\n",
      "|    iterations         | 9400      |\n",
      "|    time_elapsed       | 257       |\n",
      "|    total_timesteps    | 47000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.1     |\n",
      "|    explained_variance | 5.96e-08  |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9399      |\n",
      "|    policy_loss        | 84.3      |\n",
      "|    reward             | 1.0242362 |\n",
      "|    std                | 1.07      |\n",
      "|    value_loss         | 4.71      |\n",
      "-------------------------------------\n",
      "--------------------------------------\n",
      "| time/                 |            |\n",
      "|    fps                | 182        |\n",
      "|    iterations         | 9500       |\n",
      "|    time_elapsed       | 260        |\n",
      "|    total_timesteps    | 47500      |\n",
      "| train/                |            |\n",
      "|    entropy_loss       | -43.1      |\n",
      "|    explained_variance | 0          |\n",
      "|    learning_rate      | 0.0007     |\n",
      "|    n_updates          | 9499       |\n",
      "|    policy_loss        | 97.6       |\n",
      "|    reward             | 0.45405683 |\n",
      "|    std                | 1.07       |\n",
      "|    value_loss         | 5.39       |\n",
      "--------------------------------------\n",
      "----------------------------------------\n",
      "| time/                 |              |\n",
      "|    fps                | 182          |\n",
      "|    iterations         | 9600         |\n",
      "|    time_elapsed       | 263          |\n",
      "|    total_timesteps    | 48000        |\n",
      "| train/                |              |\n",
      "|    entropy_loss       | -43.1        |\n",
      "|    explained_variance | 0            |\n",
      "|    learning_rate      | 0.0007       |\n",
      "|    n_updates          | 9599         |\n",
      "|    policy_loss        | 78.4         |\n",
      "|    reward             | -0.105066784 |\n",
      "|    std                | 1.07         |\n",
      "|    value_loss         | 6.51         |\n",
      "----------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 182       |\n",
      "|    iterations         | 9700      |\n",
      "|    time_elapsed       | 265       |\n",
      "|    total_timesteps    | 48500     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.2     |\n",
      "|    explained_variance | -1.19e-07 |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9699      |\n",
      "|    policy_loss        | 1.88      |\n",
      "|    reward             | 0.8357275 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 1.25      |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| time/                 |          |\n",
      "|    fps                | 182      |\n",
      "|    iterations         | 9800     |\n",
      "|    time_elapsed       | 268      |\n",
      "|    total_timesteps    | 49000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -43.2    |\n",
      "|    explained_variance | 0        |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 9799     |\n",
      "|    policy_loss        | -137     |\n",
      "|    reward             | 6.046927 |\n",
      "|    std                | 1.08     |\n",
      "|    value_loss         | 24.4     |\n",
      "------------------------------------\n",
      "---------------------------------------\n",
      "| time/                 |             |\n",
      "|    fps                | 182         |\n",
      "|    iterations         | 9900        |\n",
      "|    time_elapsed       | 271         |\n",
      "|    total_timesteps    | 49500       |\n",
      "| train/                |             |\n",
      "|    entropy_loss       | -43.2       |\n",
      "|    explained_variance | 0           |\n",
      "|    learning_rate      | 0.0007      |\n",
      "|    n_updates          | 9899        |\n",
      "|    policy_loss        | 13.2        |\n",
      "|    reward             | 0.029075837 |\n",
      "|    std                | 1.08        |\n",
      "|    value_loss         | 0.219       |\n",
      "---------------------------------------\n",
      "-------------------------------------\n",
      "| time/                 |           |\n",
      "|    fps                | 182       |\n",
      "|    iterations         | 10000     |\n",
      "|    time_elapsed       | 274       |\n",
      "|    total_timesteps    | 50000     |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -43.3     |\n",
      "|    explained_variance | 0         |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 9999      |\n",
      "|    policy_loss        | 74.9      |\n",
      "|    reward             | 0.7435728 |\n",
      "|    std                | 1.08      |\n",
      "|    value_loss         | 3.96      |\n",
      "-------------------------------------\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T14:25:37.402103Z",
     "start_time": "2024-11-08T14:25:37.384025Z"
    }
   },
   "cell_type": "code",
   "source": "trained_a2c.save(TRAINED_MODEL_DIR + \"/agent_a2c\") if if_using_a2c else None",
   "id": "f444f9275aeb5b04",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Agent 2: DDPG",
   "id": "4710b0d4804a2280"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T14:27:08.075758Z",
     "start_time": "2024-11-08T14:27:08.062851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "model_ddpg = agent.get_model('ddpg')\n",
    "\n",
    "if if_using_ddpg:\n",
    "    tmp_path = RESULTS_DIR + '/ddpg'\n",
    "    new_logger_ddpg = configure(tmp_path, [\"stdout\",\"csv\",\"tensorboard\"])\n",
    "    model_ddpg.set_logger(new_logger_ddpg)"
   ],
   "id": "919d0de7bc86c988",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'buffer_size': 50000, 'learning_rate': 0.001}\n",
      "Using cpu device\n",
      "Logging to results/ddpg\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T14:42:19.359754Z",
     "start_time": "2024-11-08T14:27:39.602097Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trained_ddpg = agent.train_model(model=model_ddpg,\n",
    "                                 tb_log_name='ddpg',\n",
    "                                 total_timesteps=50000) if if_using_ddpg else None"
   ],
   "id": "83d61bd137fa5a5a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 2892, episode: 20\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2882748.73\n",
      "total_reward: 1882748.73\n",
      "total_cost: 6111.43\n",
      "total_trades: 41829\n",
      "Sharpe: 0.637\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 4        |\n",
      "|    fps             | 60       |\n",
      "|    time_elapsed    | 191      |\n",
      "|    total_timesteps | 11572    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 79.8     |\n",
      "|    critic_loss     | 9.7      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 11471    |\n",
      "|    reward          | 2.73536  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 8        |\n",
      "|    fps             | 57       |\n",
      "|    time_elapsed    | 401      |\n",
      "|    total_timesteps | 23144    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 42.6     |\n",
      "|    critic_loss     | 4.21     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 23043    |\n",
      "|    reward          | 2.73536  |\n",
      "---------------------------------\n",
      "day: 2892, episode: 30\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2804503.06\n",
      "total_reward: 1804503.06\n",
      "total_cost: 1061.59\n",
      "total_trades: 40522\n",
      "Sharpe: 0.647\n",
      "=================================\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 12       |\n",
      "|    fps             | 56       |\n",
      "|    time_elapsed    | 611      |\n",
      "|    total_timesteps | 34716    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 20.9     |\n",
      "|    critic_loss     | 1.1      |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 34615    |\n",
      "|    reward          | 2.73536  |\n",
      "---------------------------------\n",
      "---------------------------------\n",
      "| time/              |          |\n",
      "|    episodes        | 16       |\n",
      "|    fps             | 56       |\n",
      "|    time_elapsed    | 814      |\n",
      "|    total_timesteps | 46288    |\n",
      "| train/             |          |\n",
      "|    actor_loss      | 9.19     |\n",
      "|    critic_loss     | 1.15     |\n",
      "|    learning_rate   | 0.001    |\n",
      "|    n_updates       | 46187    |\n",
      "|    reward          | 2.73536  |\n",
      "---------------------------------\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T14:45:19.363361Z",
     "start_time": "2024-11-08T14:45:19.349272Z"
    }
   },
   "cell_type": "code",
   "source": "trained_ddpg.save(TRAINED_MODEL_DIR + \"/agent_ddpg\") if if_using_ddpg else None",
   "id": "ce7ed07598f3059e",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Agent 3: PPO",
   "id": "f557400049d06102"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T14:47:37.836032Z",
     "start_time": "2024-11-08T14:47:37.827798Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "PPO_PARAMS = {\n",
    "    \"n_steps\": 2048,\n",
    "    \"ent_coef\": 0.01,\n",
    "    \"learning_rate\": 0.00025,\n",
    "    \"batch_size\": 128,\n",
    "}\n",
    "\n",
    "model_ppo = agent.get_model('ppo', model_kwargs=PPO_PARAMS)\n",
    "\n",
    "if if_using_ppo:\n",
    "    tmp_path = RESULTS_DIR + '/ppo'\n",
    "    new_logger_ppo = configure(tmp_path, [\"stdout\",\"csv\",\"tensorboard\"])\n",
    "    model_ppo.set_logger(new_logger_ppo)"
   ],
   "id": "43e5c01e485d95e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_steps': 2048, 'ent_coef': 0.01, 'learning_rate': 0.00025, 'batch_size': 128}\n",
      "Using cpu device\n",
      "Logging to results/ppo\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T15:03:46.480381Z",
     "start_time": "2024-11-08T14:48:18.747465Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trained_ppo = agent.train_model(model=model_ppo,\n",
    "                                tb_log_name='ppo',\n",
    "                                total_timesteps=200000) if if_using_ppo else None"
   ],
   "id": "532a974ab9392cf5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| time/              |            |\n",
      "|    fps             | 207        |\n",
      "|    iterations      | 1          |\n",
      "|    time_elapsed    | 9          |\n",
      "|    total_timesteps | 2048       |\n",
      "| train/             |            |\n",
      "|    reward          | 0.40127242 |\n",
      "-----------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 208         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 19          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016242748 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.1       |\n",
      "|    explained_variance   | -0.0113     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 4.94        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0309     |\n",
      "|    reward               | 0.44900903  |\n",
      "|    std                  | 0.999       |\n",
      "|    value_loss           | 11.5        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 209        |\n",
      "|    iterations           | 3          |\n",
      "|    time_elapsed         | 29         |\n",
      "|    total_timesteps      | 6144       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01345348 |\n",
      "|    clip_fraction        | 0.137      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.1      |\n",
      "|    explained_variance   | 0.0092     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 32.2       |\n",
      "|    n_updates            | 20         |\n",
      "|    policy_gradient_loss | -0.0172    |\n",
      "|    reward               | -1.5470154 |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 49.9       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 210        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 38         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01502886 |\n",
      "|    clip_fraction        | 0.156      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.2      |\n",
      "|    explained_variance   | -0.0133    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 22.6       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0128    |\n",
      "|    reward               | 0.6488679  |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 53.9       |\n",
      "----------------------------------------\n",
      "day: 2892, episode: 40\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2880258.87\n",
      "total_reward: 1880258.87\n",
      "total_cost: 384567.92\n",
      "total_trades: 79972\n",
      "Sharpe: 0.680\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 211         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 48          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016354792 |\n",
      "|    clip_fraction        | 0.167       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.0054     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.67        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0265     |\n",
      "|    reward               | 0.27981687  |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 14.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 212         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018731454 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.2       |\n",
      "|    explained_variance   | -0.0134     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 17.8        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0269     |\n",
      "|    reward               | 1.3607186   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 28.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 213         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 67          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018359354 |\n",
      "|    clip_fraction        | 0.199       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.00261    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.38        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0204     |\n",
      "|    reward               | -0.46269855 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 30.6        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 213        |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 76         |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02883416 |\n",
      "|    clip_fraction        | 0.227      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.3      |\n",
      "|    explained_variance   | -0.012     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 7.96       |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0226    |\n",
      "|    reward               | -1.1444988 |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 17.7       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 86          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016146986 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.3       |\n",
      "|    explained_variance   | -0.0148     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.07        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0253     |\n",
      "|    reward               | 1.4585924   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 23          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 95          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019269835 |\n",
      "|    clip_fraction        | 0.224       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | -0.0165     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 16.1        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0186     |\n",
      "|    reward               | 0.9981253   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 24.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 214         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 104         |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020112373 |\n",
      "|    clip_fraction        | 0.277       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.4       |\n",
      "|    explained_variance   | 0.00968     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 8.48        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0162     |\n",
      "|    reward               | -0.36131406 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 27          |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 214        |\n",
      "|    iterations           | 12         |\n",
      "|    time_elapsed         | 114        |\n",
      "|    total_timesteps      | 24576      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01952596 |\n",
      "|    clip_fraction        | 0.215      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.4      |\n",
      "|    explained_variance   | 0.0237     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 3.57       |\n",
      "|    n_updates            | 110        |\n",
      "|    policy_gradient_loss | -0.0252    |\n",
      "|    reward               | 0.37255982 |\n",
      "|    std                  | 1.01       |\n",
      "|    value_loss           | 9.27       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 123         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017154694 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.5       |\n",
      "|    explained_variance   | 0.0181      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 4.41        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.0164     |\n",
      "|    reward               | -1.0709122  |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 20.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 133         |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023202933 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.5       |\n",
      "|    explained_variance   | -0.00609    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 14.5        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.0207     |\n",
      "|    reward               | -0.19207838 |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 28.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 142         |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020199148 |\n",
      "|    clip_fraction        | 0.241       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.5       |\n",
      "|    explained_variance   | -0.00126    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 8.9         |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.0171     |\n",
      "|    reward               | 3.0632665   |\n",
      "|    std                  | 1.01        |\n",
      "|    value_loss           | 19.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 152         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023729254 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.6       |\n",
      "|    explained_variance   | 0.0173      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.75        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0196     |\n",
      "|    reward               | 0.5437197   |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 28.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 161         |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021039601 |\n",
      "|    clip_fraction        | 0.229       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.6       |\n",
      "|    explained_variance   | 0.0649      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 8.11        |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.0201     |\n",
      "|    reward               | -1.3718864  |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 27.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 170         |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018082514 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.6       |\n",
      "|    explained_variance   | 0.0224      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 11.1        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.015      |\n",
      "|    reward               | 0.47480947  |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 26.7        |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 50\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1919852.97\n",
      "total_reward: 919852.97\n",
      "total_cost: 345318.69\n",
      "total_trades: 77009\n",
      "Sharpe: 0.434\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 19          |\n",
      "|    time_elapsed         | 180         |\n",
      "|    total_timesteps      | 38912       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024163127 |\n",
      "|    clip_fraction        | 0.248       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.7       |\n",
      "|    explained_variance   | 0.0115      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 4.87        |\n",
      "|    n_updates            | 180         |\n",
      "|    policy_gradient_loss | -0.0192     |\n",
      "|    reward               | -0.84137344 |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 11.8        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 216        |\n",
      "|    iterations           | 20         |\n",
      "|    time_elapsed         | 189        |\n",
      "|    total_timesteps      | 40960      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02535051 |\n",
      "|    clip_fraction        | 0.251      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -41.7      |\n",
      "|    explained_variance   | 0.0154     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 7.36       |\n",
      "|    n_updates            | 190        |\n",
      "|    policy_gradient_loss | -0.0201    |\n",
      "|    reward               | 0.14687316 |\n",
      "|    std                  | 1.02       |\n",
      "|    value_loss           | 26         |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 199         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028186468 |\n",
      "|    clip_fraction        | 0.316       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.8       |\n",
      "|    explained_variance   | 0.0227      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.14        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    reward               | -2.5457392  |\n",
      "|    std                  | 1.02        |\n",
      "|    value_loss           | 30.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 22          |\n",
      "|    time_elapsed         | 208         |\n",
      "|    total_timesteps      | 45056       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020491641 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.9       |\n",
      "|    explained_variance   | 0.0406      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.7         |\n",
      "|    n_updates            | 210         |\n",
      "|    policy_gradient_loss | -0.0175     |\n",
      "|    reward               | 0.4479603   |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 19.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 218         |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023566488 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -41.9       |\n",
      "|    explained_variance   | 0.0499      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.98        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.0171     |\n",
      "|    reward               | -1.8597872  |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 23.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 227         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022011623 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42         |\n",
      "|    explained_variance   | 0.0625      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 13          |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0214     |\n",
      "|    reward               | -0.2667349  |\n",
      "|    std                  | 1.03        |\n",
      "|    value_loss           | 32.7        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 215        |\n",
      "|    iterations           | 25         |\n",
      "|    time_elapsed         | 237        |\n",
      "|    total_timesteps      | 51200      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02363338 |\n",
      "|    clip_fraction        | 0.255      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42        |\n",
      "|    explained_variance   | 0.0689     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 19.9       |\n",
      "|    n_updates            | 240        |\n",
      "|    policy_gradient_loss | -0.0175    |\n",
      "|    reward               | -1.2612875 |\n",
      "|    std                  | 1.03       |\n",
      "|    value_loss           | 39.2       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 215        |\n",
      "|    iterations           | 26         |\n",
      "|    time_elapsed         | 246        |\n",
      "|    total_timesteps      | 53248      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03844638 |\n",
      "|    clip_fraction        | 0.283      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42        |\n",
      "|    explained_variance   | 0.0243     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 5.95       |\n",
      "|    n_updates            | 250        |\n",
      "|    policy_gradient_loss | -0.0184    |\n",
      "|    reward               | 1.0409253  |\n",
      "|    std                  | 1.03       |\n",
      "|    value_loss           | 11.8       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 215        |\n",
      "|    iterations           | 27         |\n",
      "|    time_elapsed         | 256        |\n",
      "|    total_timesteps      | 55296      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03594718 |\n",
      "|    clip_fraction        | 0.269      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.1      |\n",
      "|    explained_variance   | -0.0194    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 16.6       |\n",
      "|    n_updates            | 260        |\n",
      "|    policy_gradient_loss | -0.0131    |\n",
      "|    reward               | 0.47247183 |\n",
      "|    std                  | 1.03       |\n",
      "|    value_loss           | 24.1       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 265         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026563073 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.1       |\n",
      "|    explained_variance   | 0.0548      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 14.2        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00408    |\n",
      "|    reward               | -0.40159896 |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 39.7        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 215        |\n",
      "|    iterations           | 29         |\n",
      "|    time_elapsed         | 275        |\n",
      "|    total_timesteps      | 59392      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02489642 |\n",
      "|    clip_fraction        | 0.207      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.2      |\n",
      "|    explained_variance   | 0.0418     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 5.82       |\n",
      "|    n_updates            | 280        |\n",
      "|    policy_gradient_loss | -0.0146    |\n",
      "|    reward               | 1.6889555  |\n",
      "|    std                  | 1.04       |\n",
      "|    value_loss           | 17.9       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 284         |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020740718 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.2       |\n",
      "|    explained_variance   | 0.0743      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 23.3        |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | -0.0118     |\n",
      "|    reward               | -1.3839655  |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 36.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 294         |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017245665 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.3       |\n",
      "|    explained_variance   | 0.0997      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.27        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    reward               | -1.0567914  |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 18.3        |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 215       |\n",
      "|    iterations           | 32        |\n",
      "|    time_elapsed         | 303       |\n",
      "|    total_timesteps      | 65536     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0218353 |\n",
      "|    clip_fraction        | 0.224     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -42.3     |\n",
      "|    explained_variance   | 0.0639    |\n",
      "|    learning_rate        | 0.00025   |\n",
      "|    loss                 | 8.56      |\n",
      "|    n_updates            | 310       |\n",
      "|    policy_gradient_loss | -0.0165   |\n",
      "|    reward               | 0.4092805 |\n",
      "|    std                  | 1.04      |\n",
      "|    value_loss           | 18.3      |\n",
      "---------------------------------------\n",
      "day: 2892, episode: 60\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 1965546.43\n",
      "total_reward: 965546.43\n",
      "total_cost: 303163.97\n",
      "total_trades: 74191\n",
      "Sharpe: 0.453\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 313         |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020145979 |\n",
      "|    clip_fraction        | 0.22        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.4       |\n",
      "|    explained_variance   | 0.0501      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 13.4        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.0226     |\n",
      "|    reward               | 2.1776621   |\n",
      "|    std                  | 1.04        |\n",
      "|    value_loss           | 24.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 215         |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 322         |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.019848593 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.4       |\n",
      "|    explained_variance   | 0.0423      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 8.59        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0171     |\n",
      "|    reward               | -0.21311174 |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 20.3        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 216        |\n",
      "|    iterations           | 35         |\n",
      "|    time_elapsed         | 331        |\n",
      "|    total_timesteps      | 71680      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03359146 |\n",
      "|    clip_fraction        | 0.279      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.5      |\n",
      "|    explained_variance   | 0.123      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 15.1       |\n",
      "|    n_updates            | 340        |\n",
      "|    policy_gradient_loss | -0.0141    |\n",
      "|    reward               | -2.0983176 |\n",
      "|    std                  | 1.05       |\n",
      "|    value_loss           | 22.1       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 341         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027472917 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.5       |\n",
      "|    explained_variance   | -0.0134     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.88        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | -0.0154     |\n",
      "|    reward               | -1.9309763  |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 13          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 350         |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021947619 |\n",
      "|    clip_fraction        | 0.238       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.5       |\n",
      "|    explained_variance   | 0.0788      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 24.1        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.0205     |\n",
      "|    reward               | 0.64170784  |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 31.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 360         |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027579883 |\n",
      "|    clip_fraction        | 0.282       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | 0.0942      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 11.5        |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.0203     |\n",
      "|    reward               | -0.7052232  |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 24.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 369         |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029030086 |\n",
      "|    clip_fraction        | 0.296       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.6       |\n",
      "|    explained_variance   | 0.0969      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.91        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.0222     |\n",
      "|    reward               | -1.6467763  |\n",
      "|    std                  | 1.05        |\n",
      "|    value_loss           | 16          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 378         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029427893 |\n",
      "|    clip_fraction        | 0.281       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.7       |\n",
      "|    explained_variance   | 0.102       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.85        |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.0193     |\n",
      "|    reward               | 0.34770304  |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 27.8        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 216        |\n",
      "|    iterations           | 41         |\n",
      "|    time_elapsed         | 388        |\n",
      "|    total_timesteps      | 83968      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02614796 |\n",
      "|    clip_fraction        | 0.223      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.7      |\n",
      "|    explained_variance   | 0.07       |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 13.1       |\n",
      "|    n_updates            | 400        |\n",
      "|    policy_gradient_loss | -0.0211    |\n",
      "|    reward               | 2.2403784  |\n",
      "|    std                  | 1.06       |\n",
      "|    value_loss           | 30.7       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 397         |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020334212 |\n",
      "|    clip_fraction        | 0.228       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | 0.175       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 10.3        |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.0173     |\n",
      "|    reward               | 0.6137449   |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 22.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 407         |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.031637333 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.8       |\n",
      "|    explained_variance   | 0.0639      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.43        |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    reward               | -2.9645264  |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 13.4        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 216        |\n",
      "|    iterations           | 44         |\n",
      "|    time_elapsed         | 416        |\n",
      "|    total_timesteps      | 90112      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04220377 |\n",
      "|    clip_fraction        | 0.335      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -42.9      |\n",
      "|    explained_variance   | 0.203      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 7.82       |\n",
      "|    n_updates            | 430        |\n",
      "|    policy_gradient_loss | -0.0193    |\n",
      "|    reward               | 0.3718911  |\n",
      "|    std                  | 1.06       |\n",
      "|    value_loss           | 20.8       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 426         |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027263183 |\n",
      "|    clip_fraction        | 0.262       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -42.9       |\n",
      "|    explained_variance   | 0.0552      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 22.2        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    reward               | 3.3583758   |\n",
      "|    std                  | 1.06        |\n",
      "|    value_loss           | 42.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 435         |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032714657 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43         |\n",
      "|    explained_variance   | 0.0555      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 10.3        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | -0.00577    |\n",
      "|    reward               | -2.182095   |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 19.5        |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 70\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2123369.24\n",
      "total_reward: 1123369.24\n",
      "total_cost: 317384.77\n",
      "total_trades: 75062\n",
      "Sharpe: 0.460\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 444         |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035397753 |\n",
      "|    clip_fraction        | 0.311       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43         |\n",
      "|    explained_variance   | 0.031       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 10.1        |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.0161     |\n",
      "|    reward               | -0.978195   |\n",
      "|    std                  | 1.07        |\n",
      "|    value_loss           | 37.4        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 216        |\n",
      "|    iterations           | 48         |\n",
      "|    time_elapsed         | 454        |\n",
      "|    total_timesteps      | 98304      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02691983 |\n",
      "|    clip_fraction        | 0.272      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.1      |\n",
      "|    explained_variance   | 0.0149     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 9.2        |\n",
      "|    n_updates            | 470        |\n",
      "|    policy_gradient_loss | -0.00928   |\n",
      "|    reward               | -5.022847  |\n",
      "|    std                  | 1.07       |\n",
      "|    value_loss           | 30.1       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 463         |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034090836 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.2       |\n",
      "|    explained_variance   | 0.0361      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 23.6        |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    reward               | 0.40078282  |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 34          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 50          |\n",
      "|    time_elapsed         | 473         |\n",
      "|    total_timesteps      | 102400      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029316884 |\n",
      "|    clip_fraction        | 0.262       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.3       |\n",
      "|    explained_variance   | 0.0836      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 5.97        |\n",
      "|    n_updates            | 490         |\n",
      "|    policy_gradient_loss | -0.0199     |\n",
      "|    reward               | 0.62010884  |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 15.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 482         |\n",
      "|    total_timesteps      | 104448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027720105 |\n",
      "|    clip_fraction        | 0.284       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.4       |\n",
      "|    explained_variance   | 0.0859      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 11.3        |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.0233     |\n",
      "|    reward               | -0.7415246  |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 27.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 491         |\n",
      "|    total_timesteps      | 106496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023889892 |\n",
      "|    clip_fraction        | 0.221       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.4       |\n",
      "|    explained_variance   | 0.0438      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 14.1        |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -0.0137     |\n",
      "|    reward               | -0.30758303 |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 35.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 501         |\n",
      "|    total_timesteps      | 108544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027195992 |\n",
      "|    clip_fraction        | 0.282       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.5       |\n",
      "|    explained_variance   | 0.021       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 10.5        |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.00724    |\n",
      "|    reward               | 1.6828846   |\n",
      "|    std                  | 1.08        |\n",
      "|    value_loss           | 21          |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 216        |\n",
      "|    iterations           | 54         |\n",
      "|    time_elapsed         | 511        |\n",
      "|    total_timesteps      | 110592     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03259075 |\n",
      "|    clip_fraction        | 0.277      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.5      |\n",
      "|    explained_variance   | 0.0868     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 14         |\n",
      "|    n_updates            | 530        |\n",
      "|    policy_gradient_loss | -0.0115    |\n",
      "|    reward               | 5.6148787  |\n",
      "|    std                  | 1.09       |\n",
      "|    value_loss           | 47.6       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 216        |\n",
      "|    iterations           | 55         |\n",
      "|    time_elapsed         | 520        |\n",
      "|    total_timesteps      | 112640     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03831595 |\n",
      "|    clip_fraction        | 0.314      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.6      |\n",
      "|    explained_variance   | 0.0498     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 11.1       |\n",
      "|    n_updates            | 540        |\n",
      "|    policy_gradient_loss | -0.00768   |\n",
      "|    reward               | -4.772149  |\n",
      "|    std                  | 1.09       |\n",
      "|    value_loss           | 29.7       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 529         |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021659125 |\n",
      "|    clip_fraction        | 0.191       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.6       |\n",
      "|    explained_variance   | 0.134       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 10.5        |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.0158     |\n",
      "|    reward               | 2.3292294   |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 24.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 57          |\n",
      "|    time_elapsed         | 539         |\n",
      "|    total_timesteps      | 116736      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025077974 |\n",
      "|    clip_fraction        | 0.246       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.7       |\n",
      "|    explained_variance   | 0.192       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 22.3        |\n",
      "|    n_updates            | 560         |\n",
      "|    policy_gradient_loss | -0.0221     |\n",
      "|    reward               | 1.083053    |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 28.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 548         |\n",
      "|    total_timesteps      | 118784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022567444 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.7       |\n",
      "|    explained_variance   | 0.13        |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.03        |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0159     |\n",
      "|    reward               | 2.3627079   |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 22.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 557         |\n",
      "|    total_timesteps      | 120832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018849602 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.7       |\n",
      "|    explained_variance   | 0.0765      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 15.6        |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.00552    |\n",
      "|    reward               | -1.1030083  |\n",
      "|    std                  | 1.09        |\n",
      "|    value_loss           | 43.8        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 216        |\n",
      "|    iterations           | 60         |\n",
      "|    time_elapsed         | 567        |\n",
      "|    total_timesteps      | 122880     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03138402 |\n",
      "|    clip_fraction        | 0.305      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.7      |\n",
      "|    explained_variance   | 0.216      |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 6.46       |\n",
      "|    n_updates            | 590        |\n",
      "|    policy_gradient_loss | -0.0194    |\n",
      "|    reward               | 1.310337   |\n",
      "|    std                  | 1.09       |\n",
      "|    value_loss           | 14.1       |\n",
      "----------------------------------------\n",
      "day: 2892, episode: 80\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4025184.15\n",
      "total_reward: 3025184.15\n",
      "total_cost: 349084.57\n",
      "total_trades: 75824\n",
      "Sharpe: 0.808\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 576         |\n",
      "|    total_timesteps      | 124928      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032988984 |\n",
      "|    clip_fraction        | 0.303       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.8       |\n",
      "|    explained_variance   | 0.308       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.71        |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.0119     |\n",
      "|    reward               | 0.9571196   |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 30.1        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 216        |\n",
      "|    iterations           | 62         |\n",
      "|    time_elapsed         | 586        |\n",
      "|    total_timesteps      | 126976     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04600331 |\n",
      "|    clip_fraction        | 0.333      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -43.8      |\n",
      "|    explained_variance   | 0.0134     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 19         |\n",
      "|    n_updates            | 610        |\n",
      "|    policy_gradient_loss | 0.00414    |\n",
      "|    reward               | 2.1826024  |\n",
      "|    std                  | 1.1        |\n",
      "|    value_loss           | 71.3       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 595         |\n",
      "|    total_timesteps      | 129024      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024848282 |\n",
      "|    clip_fraction        | 0.273       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.8       |\n",
      "|    explained_variance   | 0.096       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 10.5        |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0128     |\n",
      "|    reward               | 0.6737379   |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 18.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 64          |\n",
      "|    time_elapsed         | 604         |\n",
      "|    total_timesteps      | 131072      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017977396 |\n",
      "|    clip_fraction        | 0.152       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -43.9       |\n",
      "|    explained_variance   | 0.0634      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 10.8        |\n",
      "|    n_updates            | 630         |\n",
      "|    policy_gradient_loss | -0.0206     |\n",
      "|    reward               | -1.26017    |\n",
      "|    std                  | 1.1         |\n",
      "|    value_loss           | 36          |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 216        |\n",
      "|    iterations           | 65         |\n",
      "|    time_elapsed         | 614        |\n",
      "|    total_timesteps      | 133120     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.04438303 |\n",
      "|    clip_fraction        | 0.381      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44        |\n",
      "|    explained_variance   | 0.0675     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 10.4       |\n",
      "|    n_updates            | 640        |\n",
      "|    policy_gradient_loss | -0.0162    |\n",
      "|    reward               | -0.5731014 |\n",
      "|    std                  | 1.1        |\n",
      "|    value_loss           | 46.1       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 623         |\n",
      "|    total_timesteps      | 135168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021714866 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44         |\n",
      "|    explained_variance   | 0.0422      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 33.7        |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.00615    |\n",
      "|    reward               | 0.07248247  |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 52.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 67          |\n",
      "|    time_elapsed         | 632         |\n",
      "|    total_timesteps      | 137216      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041949913 |\n",
      "|    clip_fraction        | 0.399       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.1       |\n",
      "|    explained_variance   | 0.0202      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.53        |\n",
      "|    n_updates            | 660         |\n",
      "|    policy_gradient_loss | -0.0123     |\n",
      "|    reward               | -1.81573    |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 15.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 216          |\n",
      "|    iterations           | 68           |\n",
      "|    time_elapsed         | 642          |\n",
      "|    total_timesteps      | 139264       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.032838203  |\n",
      "|    clip_fraction        | 0.244        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -44.1        |\n",
      "|    explained_variance   | 0.0392       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 20.2         |\n",
      "|    n_updates            | 670          |\n",
      "|    policy_gradient_loss | -0.0165      |\n",
      "|    reward               | -0.116461724 |\n",
      "|    std                  | 1.11         |\n",
      "|    value_loss           | 48.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 216          |\n",
      "|    iterations           | 69           |\n",
      "|    time_elapsed         | 651          |\n",
      "|    total_timesteps      | 141312       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.032989383  |\n",
      "|    clip_fraction        | 0.317        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -44.2        |\n",
      "|    explained_variance   | 0.0133       |\n",
      "|    learning_rate        | 0.00025      |\n",
      "|    loss                 | 48.6         |\n",
      "|    n_updates            | 680          |\n",
      "|    policy_gradient_loss | -0.00212     |\n",
      "|    reward               | 0.0005445479 |\n",
      "|    std                  | 1.11         |\n",
      "|    value_loss           | 92.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 661         |\n",
      "|    total_timesteps      | 143360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.03527411  |\n",
      "|    clip_fraction        | 0.319       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.2       |\n",
      "|    explained_variance   | 0.352       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 6.88        |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | -0.0151     |\n",
      "|    reward               | -0.71523786 |\n",
      "|    std                  | 1.11        |\n",
      "|    value_loss           | 17.5        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 216        |\n",
      "|    iterations           | 71         |\n",
      "|    time_elapsed         | 670        |\n",
      "|    total_timesteps      | 145408     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03661831 |\n",
      "|    clip_fraction        | 0.315      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44.2      |\n",
      "|    explained_variance   | 0.0898     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 39.4       |\n",
      "|    n_updates            | 700        |\n",
      "|    policy_gradient_loss | -0.00872   |\n",
      "|    reward               | -0.7196532 |\n",
      "|    std                  | 1.11       |\n",
      "|    value_loss           | 42.8       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 680         |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018809289 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.3       |\n",
      "|    explained_variance   | 0.0546      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 25.7        |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -0.0196     |\n",
      "|    reward               | -19.817852  |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 67.8        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 216        |\n",
      "|    iterations           | 73         |\n",
      "|    time_elapsed         | 689        |\n",
      "|    total_timesteps      | 149504     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01794457 |\n",
      "|    clip_fraction        | 0.251      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44.3      |\n",
      "|    explained_variance   | 0.0498     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 24.3       |\n",
      "|    n_updates            | 720        |\n",
      "|    policy_gradient_loss | -0.0015    |\n",
      "|    reward               | 0.8325379  |\n",
      "|    std                  | 1.12       |\n",
      "|    value_loss           | 59.8       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 74          |\n",
      "|    time_elapsed         | 699         |\n",
      "|    total_timesteps      | 151552      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024310926 |\n",
      "|    clip_fraction        | 0.237       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.3       |\n",
      "|    explained_variance   | 0.0674      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 12.8        |\n",
      "|    n_updates            | 730         |\n",
      "|    policy_gradient_loss | -0.0187     |\n",
      "|    reward               | -1.811976   |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 50.1        |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 90\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 5943383.79\n",
      "total_reward: 4943383.79\n",
      "total_cost: 253889.66\n",
      "total_trades: 69978\n",
      "Sharpe: 0.871\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 708         |\n",
      "|    total_timesteps      | 153600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.028998042 |\n",
      "|    clip_fraction        | 0.288       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.4       |\n",
      "|    explained_variance   | -0.00299    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 80.1        |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.00687    |\n",
      "|    reward               | -0.8172391  |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 102         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 76          |\n",
      "|    time_elapsed         | 718         |\n",
      "|    total_timesteps      | 155648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029439587 |\n",
      "|    clip_fraction        | 0.31        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.4       |\n",
      "|    explained_variance   | 0.0116      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 77.1        |\n",
      "|    n_updates            | 750         |\n",
      "|    policy_gradient_loss | 0.000874    |\n",
      "|    reward               | 3.4701607   |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 165         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 727         |\n",
      "|    total_timesteps      | 157696      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.032781933 |\n",
      "|    clip_fraction        | 0.346       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.5       |\n",
      "|    explained_variance   | 0.0649      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 11.8        |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.00304    |\n",
      "|    reward               | 0.33276638  |\n",
      "|    std                  | 1.12        |\n",
      "|    value_loss           | 26.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 736         |\n",
      "|    total_timesteps      | 159744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.040175784 |\n",
      "|    clip_fraction        | 0.344       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.5       |\n",
      "|    explained_variance   | 0.0434      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 45.6        |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.000785   |\n",
      "|    reward               | 0.47218314  |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 108         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 746         |\n",
      "|    total_timesteps      | 161792      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021987183 |\n",
      "|    clip_fraction        | 0.259       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.6       |\n",
      "|    explained_variance   | 0.0158      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 75.3        |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.00669    |\n",
      "|    reward               | 2.1193738   |\n",
      "|    std                  | 1.13        |\n",
      "|    value_loss           | 122         |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 216       |\n",
      "|    iterations           | 80        |\n",
      "|    time_elapsed         | 755       |\n",
      "|    total_timesteps      | 163840    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0416629 |\n",
      "|    clip_fraction        | 0.392     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -44.7     |\n",
      "|    explained_variance   | 0.0223    |\n",
      "|    learning_rate        | 0.00025   |\n",
      "|    loss                 | 16.2      |\n",
      "|    n_updates            | 790       |\n",
      "|    policy_gradient_loss | 0.00611   |\n",
      "|    reward               | 0.8200054 |\n",
      "|    std                  | 1.13      |\n",
      "|    value_loss           | 40        |\n",
      "---------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 216        |\n",
      "|    iterations           | 81         |\n",
      "|    time_elapsed         | 764        |\n",
      "|    total_timesteps      | 165888     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03641563 |\n",
      "|    clip_fraction        | 0.385      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -44.8      |\n",
      "|    explained_variance   | 0.00937    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 80.3       |\n",
      "|    n_updates            | 800        |\n",
      "|    policy_gradient_loss | -0.00436   |\n",
      "|    reward               | -1.4796398 |\n",
      "|    std                  | 1.14       |\n",
      "|    value_loss           | 85.7       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 774         |\n",
      "|    total_timesteps      | 167936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.047113836 |\n",
      "|    clip_fraction        | 0.269       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.8       |\n",
      "|    explained_variance   | 0.0193      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 23.5        |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.00335    |\n",
      "|    reward               | -1.3098626  |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 79.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 784         |\n",
      "|    total_timesteps      | 169984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.029997874 |\n",
      "|    clip_fraction        | 0.294       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.9       |\n",
      "|    explained_variance   | 0.124       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 27          |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.00171    |\n",
      "|    reward               | 0.27139395  |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 45          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 793         |\n",
      "|    total_timesteps      | 172032      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.027575202 |\n",
      "|    clip_fraction        | 0.279       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -44.9       |\n",
      "|    explained_variance   | 0.183       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 9.82        |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.015      |\n",
      "|    reward               | 0.21202192  |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 19          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 85          |\n",
      "|    time_elapsed         | 803         |\n",
      "|    total_timesteps      | 174080      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025579141 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45         |\n",
      "|    explained_variance   | 0.206       |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 12.4        |\n",
      "|    n_updates            | 840         |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    reward               | 0.39593327  |\n",
      "|    std                  | 1.14        |\n",
      "|    value_loss           | 41.2        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 216        |\n",
      "|    iterations           | 86         |\n",
      "|    time_elapsed         | 813        |\n",
      "|    total_timesteps      | 176128     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03425335 |\n",
      "|    clip_fraction        | 0.312      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -45        |\n",
      "|    explained_variance   | -0.00541   |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 24.5       |\n",
      "|    n_updates            | 850        |\n",
      "|    policy_gradient_loss | -0.00642   |\n",
      "|    reward               | 0.12456279 |\n",
      "|    std                  | 1.14       |\n",
      "|    value_loss           | 75.2       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 823         |\n",
      "|    total_timesteps      | 178176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023179758 |\n",
      "|    clip_fraction        | 0.194       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45         |\n",
      "|    explained_variance   | 0.0522      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 13.6        |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.00913    |\n",
      "|    reward               | 0.37818903  |\n",
      "|    std                  | 1.15        |\n",
      "|    value_loss           | 34.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 833         |\n",
      "|    total_timesteps      | 180224      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026356243 |\n",
      "|    clip_fraction        | 0.243       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.1       |\n",
      "|    explained_variance   | 0.0507      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 46          |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | -0.00639    |\n",
      "|    reward               | -1.122555   |\n",
      "|    std                  | 1.15        |\n",
      "|    value_loss           | 77.1        |\n",
      "-----------------------------------------\n",
      "day: 2892, episode: 100\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 4368751.86\n",
      "total_reward: 3368751.86\n",
      "total_cost: 305212.74\n",
      "total_trades: 72603\n",
      "Sharpe: 0.790\n",
      "=================================\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 842         |\n",
      "|    total_timesteps      | 182272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.041743387 |\n",
      "|    clip_fraction        | 0.361       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.1       |\n",
      "|    explained_variance   | 0.0307      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 53          |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.00641    |\n",
      "|    reward               | 0.38800266  |\n",
      "|    std                  | 1.15        |\n",
      "|    value_loss           | 92.9        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 216        |\n",
      "|    iterations           | 90         |\n",
      "|    time_elapsed         | 852        |\n",
      "|    total_timesteps      | 184320     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02040022 |\n",
      "|    clip_fraction        | 0.184      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -45.2      |\n",
      "|    explained_variance   | 0.0433     |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 30.5       |\n",
      "|    n_updates            | 890        |\n",
      "|    policy_gradient_loss | -0.0112    |\n",
      "|    reward               | 0.39527777 |\n",
      "|    std                  | 1.15       |\n",
      "|    value_loss           | 90.1       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 861         |\n",
      "|    total_timesteps      | 186368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.059973016 |\n",
      "|    clip_fraction        | 0.416       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.2       |\n",
      "|    explained_variance   | 0.000926    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 8.62        |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | 0.00364     |\n",
      "|    reward               | -4.7085752  |\n",
      "|    std                  | 1.15        |\n",
      "|    value_loss           | 21          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 871         |\n",
      "|    total_timesteps      | 188416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.034333523 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.3       |\n",
      "|    explained_variance   | 0.0275      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 96.9        |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.00482    |\n",
      "|    reward               | -1.561141   |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 136         |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| time/                   |           |\n",
      "|    fps                  | 216       |\n",
      "|    iterations           | 93        |\n",
      "|    time_elapsed         | 880       |\n",
      "|    total_timesteps      | 190464    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0305851 |\n",
      "|    clip_fraction        | 0.245     |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -45.4     |\n",
      "|    explained_variance   | 0.0535    |\n",
      "|    learning_rate        | 0.00025   |\n",
      "|    loss                 | 42.5      |\n",
      "|    n_updates            | 920       |\n",
      "|    policy_gradient_loss | -0.00322  |\n",
      "|    reward               | 1.3542234 |\n",
      "|    std                  | 1.16      |\n",
      "|    value_loss           | 70.7      |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 94          |\n",
      "|    time_elapsed         | 889         |\n",
      "|    total_timesteps      | 192512      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.035247006 |\n",
      "|    clip_fraction        | 0.365       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.4       |\n",
      "|    explained_variance   | -0.0422     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 14.9        |\n",
      "|    n_updates            | 930         |\n",
      "|    policy_gradient_loss | -0.00404    |\n",
      "|    reward               | 2.4198482   |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 32.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 899         |\n",
      "|    total_timesteps      | 194560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.054911427 |\n",
      "|    clip_fraction        | 0.336       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.4       |\n",
      "|    explained_variance   | 0.00755     |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 57.7        |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.00391    |\n",
      "|    reward               | -0.95590615 |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 143         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 96          |\n",
      "|    time_elapsed         | 908         |\n",
      "|    total_timesteps      | 196608      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025820727 |\n",
      "|    clip_fraction        | 0.24        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.5       |\n",
      "|    explained_variance   | 0.0611      |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 88.1        |\n",
      "|    n_updates            | 950         |\n",
      "|    policy_gradient_loss | -0.0019     |\n",
      "|    reward               | -1.2872198  |\n",
      "|    std                  | 1.16        |\n",
      "|    value_loss           | 101         |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 216        |\n",
      "|    iterations           | 97         |\n",
      "|    time_elapsed         | 918        |\n",
      "|    total_timesteps      | 198656     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.06341293 |\n",
      "|    clip_fraction        | 0.441      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -45.5      |\n",
      "|    explained_variance   | -0.0152    |\n",
      "|    learning_rate        | 0.00025    |\n",
      "|    loss                 | 55         |\n",
      "|    n_updates            | 960        |\n",
      "|    policy_gradient_loss | 0.0139     |\n",
      "|    reward               | -1.0682466 |\n",
      "|    std                  | 1.16       |\n",
      "|    value_loss           | 121        |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 216         |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 927         |\n",
      "|    total_timesteps      | 200704      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.024967812 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -45.5       |\n",
      "|    explained_variance   | -0.00989    |\n",
      "|    learning_rate        | 0.00025     |\n",
      "|    loss                 | 90.3        |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.00175    |\n",
      "|    reward               | 0.86904025  |\n",
      "|    std                  | 1.17        |\n",
      "|    value_loss           | 216         |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T15:05:13.200148Z",
     "start_time": "2024-11-08T15:05:13.188330Z"
    }
   },
   "cell_type": "code",
   "source": "trained_ppo.save(TRAINED_MODEL_DIR + \"/agent_ppo\") if if_using_ppo else None",
   "id": "d7fa55737f4dfd25",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Agent 4: TD3",
   "id": "99e63b99d8f20ee3"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T15:07:19.535461Z",
     "start_time": "2024-11-08T15:07:19.520768Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "TD3_PARAMS = {\n",
    "    \"batch_size\": 100,\n",
    "    \"buffer_size\": 1000000,\n",
    "    \"learning_rate\": 0.001,\n",
    "}\n",
    "model_td3 = agent.get_model('td3', model_kwargs=TD3_PARAMS)\n",
    "\n",
    "if if_using_td3:\n",
    "    tmp_path = RESULTS_DIR + '/td3'\n",
    "    new_logger_td3 = configure(tmp_path, [\"stdout\",\"csv\",\"tensorboard\"])\n",
    "    model_td3.set_logger(new_logger_td3)"
   ],
   "id": "cd86f1d6666bc6e3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 100, 'buffer_size': 1000000, 'learning_rate': 0.001}\n",
      "Using cpu device\n",
      "Logging to results/td3\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T15:16:45.188692Z",
     "start_time": "2024-11-08T15:08:05.833861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trained_td3 = agent.train_model(model=model_td3,\n",
    "                                tb_log_name='td3',\n",
    "                                total_timesteps=50000) if if_using_td3 else None"
   ],
   "id": "643d5f2975a92052",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day: 2892, episode: 110\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3618263.14\n",
      "total_reward: 2618263.14\n",
      "total_cost: 1090.72\n",
      "total_trades: 49181\n",
      "Sharpe: 0.733\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 96        |\n",
      "|    time_elapsed    | 120       |\n",
      "|    total_timesteps | 11572     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 242       |\n",
      "|    critic_loss     | 125       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 11471     |\n",
      "|    reward          | 4.5480433 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 96        |\n",
      "|    time_elapsed    | 239       |\n",
      "|    total_timesteps | 23144     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 131       |\n",
      "|    critic_loss     | 928       |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 23043     |\n",
      "|    reward          | 4.5480433 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 96        |\n",
      "|    time_elapsed    | 359       |\n",
      "|    total_timesteps | 34716     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 114       |\n",
      "|    critic_loss     | 13.2      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 34615     |\n",
      "|    reward          | 4.5480433 |\n",
      "----------------------------------\n",
      "day: 2892, episode: 120\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3618263.14\n",
      "total_reward: 2618263.14\n",
      "total_cost: 1090.72\n",
      "total_trades: 49181\n",
      "Sharpe: 0.733\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 96        |\n",
      "|    time_elapsed    | 480       |\n",
      "|    total_timesteps | 46288     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 98.4      |\n",
      "|    critic_loss     | 9.17      |\n",
      "|    learning_rate   | 0.001     |\n",
      "|    n_updates       | 46187     |\n",
      "|    reward          | 4.5480433 |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T15:18:19.036003Z",
     "start_time": "2024-11-08T15:18:19.015074Z"
    }
   },
   "cell_type": "code",
   "source": "trained_td3.save(TRAINED_MODEL_DIR + \"/agent_td3\") if if_using_td3 else None",
   "id": "c9f88ca359fea062",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Agent 5: SAC",
   "id": "d57b4fa2ff99ff77"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T15:20:27.054266Z",
     "start_time": "2024-11-08T15:20:27.042556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "agent = DRLAgent(env = env_train)\n",
    "SAC_PARAMS = {\n",
    "    \"batch_size\": 128,\n",
    "    \"buffer_size\": 100000,\n",
    "    \"learning_rate\": 0.0001,\n",
    "    \"learning_starts\": 100,\n",
    "    \"ent_coef\": \"auto_0.1\",\n",
    "}\n",
    "model_sac = agent.get_model('sac', model_kwargs=SAC_PARAMS)\n",
    "\n",
    "if if_using_sac:\n",
    "    tmp_path = RESULTS_DIR + '/sac'\n",
    "    new_logger_sac = configure(tmp_path, [\"stdout\",\"csv\",\"tensorboard\"])\n",
    "    model_sac.set_logger(new_logger_sac)"
   ],
   "id": "8ff0817ad8db33ce",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 128, 'buffer_size': 100000, 'learning_rate': 0.0001, 'learning_starts': 100, 'ent_coef': 'auto_0.1'}\n",
      "Using cpu device\n",
      "Logging to results/sac\n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T15:34:38.981912Z",
     "start_time": "2024-11-08T15:20:58.255242Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trained_sac = agent.train_model(model=model_sac,\n",
    "                                tb_log_name='sac',\n",
    "                                total_timesteps=70000) if if_using_sac else None"
   ],
   "id": "af17f486e8a7f14d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 4         |\n",
      "|    fps             | 85        |\n",
      "|    time_elapsed    | 135       |\n",
      "|    total_timesteps | 11572     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 860       |\n",
      "|    critic_loss     | 119       |\n",
      "|    ent_coef        | 0.168     |\n",
      "|    ent_coef_loss   | -83.2     |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 11471     |\n",
      "|    reward          | 6.1221814 |\n",
      "----------------------------------\n",
      "day: 2892, episode: 130\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 2349834.06\n",
      "total_reward: 1349834.06\n",
      "total_cost: 131232.15\n",
      "total_trades: 66074\n",
      "Sharpe: 0.540\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 8         |\n",
      "|    fps             | 85        |\n",
      "|    time_elapsed    | 271       |\n",
      "|    total_timesteps | 23144     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 372       |\n",
      "|    critic_loss     | 30.9      |\n",
      "|    ent_coef        | 0.054     |\n",
      "|    ent_coef_loss   | -111      |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 23043     |\n",
      "|    reward          | 1.0911896 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 12        |\n",
      "|    fps             | 85        |\n",
      "|    time_elapsed    | 406       |\n",
      "|    total_timesteps | 34716     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 190       |\n",
      "|    critic_loss     | 10.2      |\n",
      "|    ent_coef        | 0.0174    |\n",
      "|    ent_coef_loss   | -127      |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 34615     |\n",
      "|    reward          | 2.7145016 |\n",
      "----------------------------------\n",
      "day: 2892, episode: 140\n",
      "begin_total_asset: 1000000.00\n",
      "end_total_asset: 3057553.08\n",
      "total_reward: 2057553.08\n",
      "total_cost: 4562.94\n",
      "total_trades: 50194\n",
      "Sharpe: 0.672\n",
      "=================================\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 16        |\n",
      "|    fps             | 85        |\n",
      "|    time_elapsed    | 542       |\n",
      "|    total_timesteps | 46288     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 102       |\n",
      "|    critic_loss     | 5.93      |\n",
      "|    ent_coef        | 0.00562   |\n",
      "|    ent_coef_loss   | -114      |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 46187     |\n",
      "|    reward          | 3.1472251 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 20        |\n",
      "|    fps             | 85        |\n",
      "|    time_elapsed    | 680       |\n",
      "|    total_timesteps | 57860     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 57.5      |\n",
      "|    critic_loss     | 3.99      |\n",
      "|    ent_coef        | 0.00195   |\n",
      "|    ent_coef_loss   | -48.7     |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 57759     |\n",
      "|    reward          | 3.0579891 |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| time/              |           |\n",
      "|    episodes        | 24        |\n",
      "|    fps             | 85        |\n",
      "|    time_elapsed    | 814       |\n",
      "|    total_timesteps | 69432     |\n",
      "| train/             |           |\n",
      "|    actor_loss      | 34.3      |\n",
      "|    critic_loss     | 6.39      |\n",
      "|    ent_coef        | 0.00133   |\n",
      "|    ent_coef_loss   | 0.845     |\n",
      "|    learning_rate   | 0.0001    |\n",
      "|    n_updates       | 69331     |\n",
      "|    reward          | 3.0875645 |\n",
      "----------------------------------\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-08T15:35:47.941852Z",
     "start_time": "2024-11-08T15:35:47.927577Z"
    }
   },
   "cell_type": "code",
   "source": "trained_sac.save(TRAINED_MODEL_DIR + \"/agent_sac\") if if_using_sac else None",
   "id": "3eaed38dbf1dc275",
   "outputs": [],
   "execution_count": 32
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
